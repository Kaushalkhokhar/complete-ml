{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "69a732b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1b17d76f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow import keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1274560c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import fetch_california_housing\n",
    "\n",
    "dataset = fetch_california_housing()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "96df14d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = np.c_[dataset.data, dataset.target]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "c4fba2d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "train_full, test = train_test_split(data, test_size=0.1, random_state=42)\n",
    "train, valid = train_test_split(train_full, test_size=0.1, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "c6dba5c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train = pd.DataFrame(train, columns=dataset.feature_names + dataset.target_names)\n",
    "df_valid = pd.DataFrame(valid, columns=dataset.feature_names + dataset.target_names)\n",
    "df_test = pd.DataFrame(test, columns=dataset.feature_names + dataset.target_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "56887275",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'D:\\\\TheCompleteML\\\\projects'"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "if \"ANN\" in os.path.abspath(os.curdir): os.chdir(\"..\")\n",
    "BASE_DIR = os.path.abspath(os.curdir)\n",
    "BASE_DIR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "5b1391cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_and_save(df, split_count, target_dir, prefix):\n",
    "    for i in range(split_count):\n",
    "        df_ = df[i*int(df.shape[0]//15):(i+1)*int(df.shape[0]//15)]\n",
    "        os.makedirs(target_dir, exist_ok=True)\n",
    "        df_.to_csv(os.path.join(target_dir, \"{}_{}.csv\".format(prefix, i+1)), index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "id": "868e9997",
   "metadata": {},
   "outputs": [],
   "source": [
    "for prefix in [\"train\", \"valid\", \"test\"]:\n",
    "    split_count = 15\n",
    "    target_dir = os.path.join(BASE_DIR, \"datasets\", \"ann\", prefix)\n",
    "    split_and_save(df_valid, split_count, target_dir, prefix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "id": "f5774b9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_filepaths = [f'{os.path.join(BASE_DIR, \"datasets\", \"ann\", \"train\")}\\\\train_{i+1}.csv' for i in range(15)]\n",
    "test_filepaths = [f'{os.path.join(BASE_DIR, \"datasets\", \"ann\", \"test\")}\\\\test_{i+1}.csv' for i in range(15)]\n",
    "valid_filepaths = [f'{os.path.join(BASE_DIR, \"datasets\", \"ann\", \"valid\")}\\\\valid_{i+1}.csv' for i in range(15)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "id": "6c5100a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "filepath_dataset = tf.data.Dataset.list_files(train_filepaths, seed=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "id": "9e445629",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor(b'D:\\\\TheCompleteML\\\\projects\\\\datasets\\\\ann\\\\train\\\\train_1.csv', shape=(), dtype=string)\n",
      "tf.Tensor(b'D:\\\\TheCompleteML\\\\projects\\\\datasets\\\\ann\\\\train\\\\train_13.csv', shape=(), dtype=string)\n",
      "tf.Tensor(b'D:\\\\TheCompleteML\\\\projects\\\\datasets\\\\ann\\\\train\\\\train_7.csv', shape=(), dtype=string)\n",
      "tf.Tensor(b'D:\\\\TheCompleteML\\\\projects\\\\datasets\\\\ann\\\\train\\\\train_2.csv', shape=(), dtype=string)\n",
      "tf.Tensor(b'D:\\\\TheCompleteML\\\\projects\\\\datasets\\\\ann\\\\train\\\\train_6.csv', shape=(), dtype=string)\n",
      "tf.Tensor(b'D:\\\\TheCompleteML\\\\projects\\\\datasets\\\\ann\\\\train\\\\train_4.csv', shape=(), dtype=string)\n",
      "tf.Tensor(b'D:\\\\TheCompleteML\\\\projects\\\\datasets\\\\ann\\\\train\\\\train_9.csv', shape=(), dtype=string)\n",
      "tf.Tensor(b'D:\\\\TheCompleteML\\\\projects\\\\datasets\\\\ann\\\\train\\\\train_14.csv', shape=(), dtype=string)\n",
      "tf.Tensor(b'D:\\\\TheCompleteML\\\\projects\\\\datasets\\\\ann\\\\train\\\\train_10.csv', shape=(), dtype=string)\n",
      "tf.Tensor(b'D:\\\\TheCompleteML\\\\projects\\\\datasets\\\\ann\\\\train\\\\train_11.csv', shape=(), dtype=string)\n",
      "tf.Tensor(b'D:\\\\TheCompleteML\\\\projects\\\\datasets\\\\ann\\\\train\\\\train_3.csv', shape=(), dtype=string)\n",
      "tf.Tensor(b'D:\\\\TheCompleteML\\\\projects\\\\datasets\\\\ann\\\\train\\\\train_8.csv', shape=(), dtype=string)\n",
      "tf.Tensor(b'D:\\\\TheCompleteML\\\\projects\\\\datasets\\\\ann\\\\train\\\\train_5.csv', shape=(), dtype=string)\n",
      "tf.Tensor(b'D:\\\\TheCompleteML\\\\projects\\\\datasets\\\\ann\\\\train\\\\train_15.csv', shape=(), dtype=string)\n",
      "tf.Tensor(b'D:\\\\TheCompleteML\\\\projects\\\\datasets\\\\ann\\\\train\\\\train_12.csv', shape=(), dtype=string)\n"
     ]
    }
   ],
   "source": [
    "for i in filepath_dataset:\n",
    "    print(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "id": "d9db5c01",
   "metadata": {},
   "outputs": [],
   "source": [
    "n_readers = 5\n",
    "dataset = filepath_dataset.interleave(lambda filepath: tf.data.TextLineDataset(filepath).skip(1), \n",
    "                                      cycle_length=n_readers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "id": "484f7430",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor(b'5.1482,12.0,6.781582054309327,1.244391971664699,2104.0,2.4840613931523023,33.38,-117.63,5.00001', shape=(), dtype=string)\n",
      "tf.Tensor(b'9.2327,19.0,8.118279569892474,1.014336917562724,904.0,3.240143369175627,33.88,-117.81,4.613', shape=(), dtype=string)\n",
      "tf.Tensor(b'6.2242,13.0,6.121320890165111,0.9791816223977028,4597.0,3.3000717875089736,33.65,-117.66,2.379', shape=(), dtype=string)\n"
     ]
    }
   ],
   "source": [
    "for item in dataset.take(3):\n",
    "    print(item)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "id": "5ec44af5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(154.2774108932893, 592.7775189615072)"
      ]
     },
     "execution_count": 146,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "n_inputs = 8\n",
    "X_mean = np.mean(train[:-1])\n",
    "X_std = np.std(train[:-1])\n",
    "X_mean, X_std"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52d28fd7",
   "metadata": {},
   "source": [
    "### Loading, shuffling and preprocessing\n",
    "\n",
    "Here main focus is on shufflig the data and also little bit of preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "id": "1b28ece3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess(line):\n",
    "    defs = [0.] * n_inputs + [tf.constant([], dtype=tf.float32)]\n",
    "    fields = tf.io.decode_csv(line, record_defaults=defs)\n",
    "    X = tf.stack(fields[:-1])\n",
    "    y = tf.stack(fields[-1:])\n",
    "    return (X - X_mean) / X_std, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "id": "8b552a2e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(<tf.Tensor: shape=(8,), dtype=float32, numpy=\n",
       " array([-0.25157702, -0.24001822, -0.24882154, -0.25816265,  3.2891304 ,\n",
       "        -0.25607136, -0.20395072, -0.4587006 ], dtype=float32)>,\n",
       " <tf.Tensor: shape=(1,), dtype=float32, numpy=array([5.00001], dtype=float32)>)"
      ]
     },
     "execution_count": 152,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "preprocess(b'5.1482,12.0,6.781582054309327,1.244391971664699,2104.0,2.4840613931523023,33.38,-117.63,5.00001')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "id": "8d26d117",
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_csv_dataset(filepaths, repeat=1, n_readers=5, n_read_threads=None, \n",
    "                     shuffle_buffer_size=10000, n_parse_threads=5, batch_size=32):\n",
    "    dataset = tf.data.Dataset.list_files(filepaths)\n",
    "    dataset = dataset.interleave(lambda filepath: tf.data.TextLineDataset(filepath).skip(1), \n",
    "                                 cycle_length=n_readers,\n",
    "                                 num_parallel_calls=n_read_threads)\n",
    "    dataset = dataset.map(preprocess, num_parallel_calls=n_parse_threads)\n",
    "    dataset = dataset.shuffle(shuffle_buffer_size).repeat(repeat)\n",
    "    return dataset.batch(batch_size).prefetch(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "334691b6",
   "metadata": {},
   "source": [
    "#### Demo to use train, valid and test set in keras API's"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "id": "4191ad87",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_set = read_csv_dataset(train_filepaths)\n",
    "valid_set = read_csv_dataset(valid_filepaths)\n",
    "test_set = read_csv_dataset(test_filepaths)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "id": "d95c6e87",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = keras.models.Sequential()\n",
    "model.add(keras.layers.Flatten(input_shape=[8]))\n",
    "model.add(keras.layers.Dense(100, activation=\"relu\"))\n",
    "model.add(keras.layers.Dense(1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "id": "d8357b07",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(loss=keras.losses.mean_squared_error,\n",
    "             optimizer=\"sgd\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "id": "36151b70",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/3\n",
      "58/58 [==============================] - 1s 8ms/step - loss: 1.9158 - val_loss: 1.4521\n",
      "Epoch 2/3\n",
      "58/58 [==============================] - 0s 3ms/step - loss: 1.3347 - val_loss: 1.3074\n",
      "Epoch 3/3\n",
      "58/58 [==============================] - 0s 3ms/step - loss: 1.3213 - val_loss: 1.3087\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x1ea52f65040>"
      ]
     },
     "execution_count": 180,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(train_set, epochs=3, validation_data=valid_set)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe4d6083",
   "metadata": {},
   "source": [
    "### Preprocessing the input features\n",
    "\n",
    "Here focus is to create a layer that is responsible from preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 190,
   "id": "aeea6ec7",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Standardization(keras.layers.Layer):\n",
    "    def adapt(self, data_sample):\n",
    "        self.mean = np.mean(data_sample, axis=0, keepdims=True)\n",
    "        self.std = np.std(data_sample, axis=0, keepdims=True)\n",
    "        self.eps = keras.backend.epsilon()\n",
    "    \n",
    "    def call(self, inputs):\n",
    "        return (inputs - self.mean) / (self.std + self.eps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 191,
   "id": "e535b611",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_sample = np.array(pd.read_csv(train_filepaths[0]).iloc[:, :-1])\n",
    "std_layer = Standardization()\n",
    "std_layer.adapt(data_sample)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 196,
   "id": "3f96d429",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/3\n",
      "58/58 [==============================] - 1s 5ms/step - loss: 3611320.5000 - val_loss: 9008.5498\n",
      "Epoch 2/3\n",
      "58/58 [==============================] - 0s 3ms/step - loss: 3561.8923 - val_loss: 865.2935\n",
      "Epoch 3/3\n",
      "58/58 [==============================] - 0s 3ms/step - loss: 342.9004 - val_loss: 84.2560\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x1ea51cd1f40>"
      ]
     },
     "execution_count": 196,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "keras.backend.clear_session()\n",
    "tf.random.set_seed(42)\n",
    "np.random.seed(42)\n",
    "\n",
    "model = keras.models.Sequential()\n",
    "model.add(std_layer) # our preprocessing layer\n",
    "model.add(keras.layers.Flatten(input_shape=[8]))\n",
    "model.add(keras.layers.Dense(100, activation=\"relu\"))\n",
    "model.add(keras.layers.Dense(1))\n",
    "\n",
    "model.compile(loss=keras.losses.mean_squared_error,\n",
    "             optimizer=\"sgd\")\n",
    "\n",
    "model.fit(train_set, epochs=3, validation_data=valid_set)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f57b595c",
   "metadata": {},
   "source": [
    "### Encoding categorical to one-hot-encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 199,
   "id": "e3c6d7a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab = [\"< 1H OCEAN\", \"INLAND\", \"NEAR OCEAN\", \"NEAR BAY\", \"ISLAND\"]\n",
    "indices = tf.range(len(vocab), dtype=tf.int64)\n",
    "\n",
    "table_init = tf.lookup.KeyValueTensorInitializer(vocab, indices)\n",
    "num_oov_buckets = 2\n",
    "table = tf.lookup.StaticVocabularyTable(table_init, num_oov_buckets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 200,
   "id": "8aeebe35",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(4,), dtype=string, numpy=array([b'NEAR BAY', b'DESERT', b'INLAND', b'INLAND'], dtype=object)>"
      ]
     },
     "execution_count": 200,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "categories = tf.constant([\"NEAR BAY\", \"DESERT\", \"INLAND\", \"INLAND\"])\n",
    "categories"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 201,
   "id": "695dfb0d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(4,), dtype=int64, numpy=array([3, 5, 1, 1], dtype=int64)>"
      ]
     },
     "execution_count": 201,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cat_indices = table.lookup(categories)\n",
    "cat_indices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 204,
   "id": "e36231f3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(4, 7), dtype=float32, numpy=\n",
       "array([[0., 0., 0., 1., 0., 0., 0.],\n",
       "       [0., 0., 0., 0., 0., 1., 0.],\n",
       "       [0., 1., 0., 0., 0., 0., 0.],\n",
       "       [0., 1., 0., 0., 0., 0., 0.]], dtype=float32)>"
      ]
     },
     "execution_count": 204,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cat_one_hot = tf.one_hot(cat_indices, depth=len(vocab) + num_oov_buckets)\n",
    "cat_one_hot"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf0c1095",
   "metadata": {},
   "source": [
    "### Encoding categorical to embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 207,
   "id": "9c377419",
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_dim = 2\n",
    "embed_init = tf.random.uniform([len(vocab) + num_oov_buckets, embedding_dim])\n",
    "embedding_matrix = tf.Variable(embed_init)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 209,
   "id": "e35f1bb8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Variable 'Variable:0' shape=(7, 2) dtype=float32, numpy=\n",
       "array([[0.7413678 , 0.62854624],\n",
       "       [0.01738465, 0.3431449 ],\n",
       "       [0.51063764, 0.3777541 ],\n",
       "       [0.07321596, 0.02137029],\n",
       "       [0.2871771 , 0.4710616 ],\n",
       "       [0.6936141 , 0.07321334],\n",
       "       [0.93251204, 0.20843053]], dtype=float32)>"
      ]
     },
     "execution_count": 209,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embedding_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 211,
   "id": "d9ececce",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(4,), dtype=int64, numpy=array([3, 5, 1, 1], dtype=int64)>"
      ]
     },
     "execution_count": 211,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cat_indices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 210,
   "id": "266bc136",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(4, 2), dtype=float32, numpy=\n",
       "array([[0.07321596, 0.02137029],\n",
       "       [0.6936141 , 0.07321334],\n",
       "       [0.01738465, 0.3431449 ],\n",
       "       [0.01738465, 0.3431449 ]], dtype=float32)>"
      ]
     },
     "execution_count": 210,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.nn.embedding_lookup(embedding_matrix, cat_indices)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dade4f41",
   "metadata": {},
   "source": [
    "#### Using keras embedding layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 212,
   "id": "c53ea66f",
   "metadata": {},
   "outputs": [],
   "source": [
    "regular_inputs = keras.layers.Input(shape=[8], dtype=tf.float32)\n",
    "categorical_inputs = keras.layers.Input(shape=[], dtype=tf.string)\n",
    "cat_indices = keras.layers.Lambda(lambda cats: table.lookup(cats))(categorical_inputs)\n",
    "cat_embed = keras.layers.Embedding(input_dim=6, output_dim=2)(cat_indices)\n",
    "\n",
    "encoded_inputs = keras.layers.concatenate([regular_inputs, cat_embed])\n",
    "output = keras.layers.Dense(1)(encoded_inputs)\n",
    "\n",
    "model = keras.models.Model(inputs=[regular_inputs, categorical_inputs], outputs=[output])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "750cc3b6",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
