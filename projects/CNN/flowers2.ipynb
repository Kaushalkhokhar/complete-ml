{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "eb75d6f1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "232"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import gc\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "206dce00",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib as mpl\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "285aa102",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow import keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "5aede7ea",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of available GPU's:  0\n"
     ]
    }
   ],
   "source": [
    "print(\"Number of available GPU's: \", len(tf.config.experimental.list_physical_devices(\"GPU\")))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "3343bb30",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'D:\\\\TheCompleteML\\\\projects'"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "if \"CNN\" in os.path.abspath(os.curdir): os.chdir(\"..\")\n",
    "BASE_DIR = os.path.abspath(os.curdir)\n",
    "BASE_DIR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "61a5f18f",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dir = os.path.join(BASE_DIR, \"datasets\", \"classification\", \"flowers\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "3d842315",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['D:\\\\TheCompleteML\\\\projects\\\\datasets\\\\classification\\\\flowers\\\\daisy',\n",
       " 'D:\\\\TheCompleteML\\\\projects\\\\datasets\\\\classification\\\\flowers\\\\dandelion',\n",
       " 'D:\\\\TheCompleteML\\\\projects\\\\datasets\\\\classification\\\\flowers\\\\rose',\n",
       " 'D:\\\\TheCompleteML\\\\projects\\\\datasets\\\\classification\\\\flowers\\\\sunflower',\n",
       " 'D:\\\\TheCompleteML\\\\projects\\\\datasets\\\\classification\\\\flowers\\\\tulip']"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_dirs = [os.path.join(data_dir, dir_) for dir_ in os.listdir(data_dir) if \"processed\" not in dir_]\n",
    "data_dirs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46f5974d",
   "metadata": {},
   "source": [
    "## Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "0e76a2c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "from skimage import io\n",
    "\n",
    "class loading_and_splitting:\n",
    "    \n",
    "    def __init__(self, data_dirs, dims, channels=3, target_dir=data_dir):\n",
    "        self.total_images = 0\n",
    "        self.minh = np.inf\n",
    "        self.minw = np.inf\n",
    "        self.dims = dims\n",
    "        self.channels = channels\n",
    "        self.target_dir = target_dir\n",
    "        self.data_dirs = data_dirs\n",
    "        self.class_map = {k:v.split(\"\\\\\")[-1] for k, v in enumerate(data_dirs)}\n",
    "        \n",
    "        self.header_list = [f\"x{i}\" for i in range(self.dims[0]*self.dims[1]*self.channels)] + [\"label\"]\n",
    "        self.sample_list = [random.sample(range(len(os.listdir(path))), \n",
    "                                          len(os.listdir(path))) for path in data_dirs]\n",
    "        for item in self.sample_list:\n",
    "            self.total_images += len(item)\n",
    "        self.generate_samples()\n",
    "        self.max_train_instance = len(self.train_seq)\n",
    "        self.max_valid_instance = len(self.valid_seq)\n",
    "        self.max_test_instance = len(self.test_seq)\n",
    "    \n",
    "    def generate_csvs(self):\n",
    "        header_list = [f\"x{i}\" for i in range(self.dims[0]*self.dims[1]*self.channels)] + [\"label\"]\n",
    "        for set_ in [\"train\", \"valid\", \"test\"]:\n",
    "            with open(os.path.join(self.target_dir, f\"{set_}.csv\"), \"w\") as f:\n",
    "                df = pd.DataFrame(list(), columns=header_list)\n",
    "                df.to_csv(f, index=False)\n",
    "                \n",
    "    def generate_samples(self):\n",
    "        self.sample_seq = random.sample(range(self.total_images), self.total_images)\n",
    "        self.train_seq = self.sample_seq[:int(len(self.sample_seq)*0.8)]\n",
    "        self.valid_seq = self.sample_seq[int(len(self.sample_seq)*0.8):int(len(self.sample_seq)*0.9)]\n",
    "        self.test_seq = self.sample_seq[int(len(self.sample_seq)*0.9):]\n",
    "    \n",
    "    def crop_image(self, image):\n",
    "        h, w, d = image.shape\n",
    "        if h >= self.minh and w >= self.minw:\n",
    "            image = image[int(h/2)-64:int(h/2)+64, \n",
    "                          int(w/2)-64:int(w/2)+64, \n",
    "                          :]\n",
    "            return image\n",
    "    \n",
    "    def crop_or_pad(self, image):\n",
    "        image = tf.image.resize_with_crop_or_pad(image, self.dims[0], self.dims[0])\n",
    "        return image.numpy()\n",
    "    \n",
    "    def shuffle_and_save(self):\n",
    "        empty = []\n",
    "        train = np.zeros((1, self.dims[0]*self.dims[1]*self.channels + 1))\n",
    "        valid = np.zeros((1, self.dims[0]*self.dims[1]*self.channels + 1))\n",
    "        test = np.zeros((1, self.dims[0]*self.dims[1]*self.channels + 1))\n",
    "        count = 0\n",
    "        while len(empty) != len(self.data_dirs):\n",
    "            sel_dir = np.random.randint(0, len(self.data_dirs))\n",
    "            if sel_dir in empty: continue\n",
    "            dir_ = self.data_dirs[sel_dir]\n",
    "            if not self.sample_list[sel_dir]:\n",
    "                empty.append(sel_dir)\n",
    "            else:\n",
    "                count += 1\n",
    "                print(f\"Processing: {count}\")\n",
    "                sel_image = self.sample_list[sel_dir].pop()\n",
    "                image = io.imread(os.path.join(dir_, os.listdir(dir_)[sel_image]))\n",
    "                \n",
    "                h, w, d = image.shape\n",
    "                if h < self.minh: self.minh = h\n",
    "                if w < self.minw: self.minw = w\n",
    "                if self.minh < self.dims[0]: self.minh = self.dims[0]\n",
    "                if self.minw < self.dims[1]: self.minw = self.dims[1]\n",
    "                \n",
    "                # image = self.crop_image(image)\n",
    "                image = self.crop_or_pad(image)\n",
    "                \n",
    "                if not isinstance(image, np.ndarray): continue\n",
    "                if sel_image in self.train_seq: \n",
    "                    train = np.append(train, np.append(image.flatten(), sel_dir).reshape(1,-1), axis=0)\n",
    "                elif sel_image in self.valid_seq: \n",
    "                    valid = np.append(valid, np.append(image.flatten(), sel_dir).reshape(1,-1), axis=0)\n",
    "                elif sel_image in self.test_seq: \n",
    "                    test = np.append(test, np.append(image.flatten(), sel_dir).reshape(1,-1), axis=0)\n",
    "        \n",
    "        train = train[1:, :]\n",
    "        valid = valid[1:, :]\n",
    "        test = test[1:, :]\n",
    "        \n",
    "        for prefix, arr in zip([\"train\", \"valid\", \"test\"], [train, valid, test]):\n",
    "            self.split_and_save(arr, os.path.join(self.target_dir, \"processed\", prefix), prefix)\n",
    "        \n",
    "    def split_and_save(self, arr, target_dir, prefix, split_count=10):\n",
    "        os.makedirs(target_dir, exist_ok=True)\n",
    "        for i in range(split_count):\n",
    "            df = pd.DataFrame(arr[i*int(arr.shape[0]//split_count):(i+1)*int(arr.shape[0]//split_count), :], \n",
    "                             columns=self.header_list)\n",
    "            df.to_csv(os.path.join(target_dir, \"{}_{}.csv\".format(prefix, i+1)), index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "02212dd4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{0: 'daisy', 1: 'dandelion', 2: 'rose', 3: 'sunflower', 4: 'tulip'}"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dims = (150, 150)\n",
    "channels = 3\n",
    "n_features = dims[0] * dims[1] * channels\n",
    "ls = loading_and_splitting(data_dirs=data_dirs, dims=dims, channels=channels, target_dir=data_dir)\n",
    "class_map = ls.class_map\n",
    "max_train_instance = ls.max_train_instance\n",
    "max_valid_instance = ls.max_valid_instance\n",
    "max_test_instance = ls.max_test_instance\n",
    "class_map\n",
    "# ls.shuffle_and_save()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "6545f4cc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(3453, 432, 432)"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "max_train_instance, max_valid_instance, max_test_instance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "1692970b",
   "metadata": {},
   "outputs": [],
   "source": [
    "set_dir = os.path.join(data_dir, \"processed\")\n",
    "train_paths = [f\"{os.path.join(set_dir, 'train')}\\\\{item}\" for item in os.listdir(os.path.join(set_dir, \"train\"))]\n",
    "valid_paths = [f\"{os.path.join(set_dir, 'valid')}\\\\{item}\" for item in os.listdir(os.path.join(set_dir, \"valid\"))]\n",
    "test_paths = [f\"{os.path.join(set_dir, 'test')}\\\\{item}\" for item in os.listdir(os.path.join(set_dir, \"test\"))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "1aa4a538",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_augmentation = tf.keras.Sequential([keras.layers.RandomFlip(\"horizontal_and_vertical\"), \n",
    "                                         keras.layers.RandomRotation(0.2),\n",
    "                                         keras.layers.RandomContrast(0.5),\n",
    "                                         keras.layers.RandomZoom((-0.3, 0.3), (-0.3, 0.3))\n",
    "                                         ])\n",
    "\n",
    "def preprocess(line, augmentation=False):\n",
    "    defs = [tf.constant([], dtype = tf.float32)] * (n_features + 1)\n",
    "    xy = tf.io.decode_csv(line, record_defaults=defs)\n",
    "    X = tf.stack(xy[:-1])\n",
    "    y = tf.stack(xy[-1:])\n",
    "    \n",
    "    # prcessing steps\n",
    "    X = tf.divide(X, 255)\n",
    "    X = tf.reshape(X, [dims[0], dims[1], channels])\n",
    "    if augmnentation:\n",
    "        X = data_augmentation(X)\n",
    "        X = tf.image.rot90(X)\n",
    "        X = tf.image.random_brightness(X, 0.2)\n",
    "    \n",
    "    return X, y\n",
    "\n",
    "\n",
    "def preprocess_test(X):\n",
    "    # prcessing steps\n",
    "    \n",
    "    X = tf.image.resize_with_crop_or_pad(X, 150 ,150)\n",
    "    X = data_augmentation(X)\n",
    "    X = tf.image.rot90(X)\n",
    "    X = tf.image.random_brightness(X, 0.2)\n",
    "    X = tf.divide(X, 255)\n",
    "    \n",
    "    return X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "253f8beb",
   "metadata": {},
   "outputs": [],
   "source": [
    "from functools import partial\n",
    "\n",
    "def read_csv_pipeline(paths, \n",
    "                      n_readers, \n",
    "                      shuffle_buffer_size, \n",
    "                      n_read_threds, \n",
    "                      n_parse_threads, \n",
    "                      batch_size, \n",
    "                      augmentation=False):\n",
    "    \n",
    "    filepaths = tf.data.Dataset.list_files(paths, seed=42)\n",
    "    dataset = filepaths.interleave(lambda filepath: tf.data.TextLineDataset(filepath).skip(1), cycle_length=n_readers)\n",
    "    dataset = dataset.shuffle(shuffle_buffer_size)\n",
    "    if augmentation: dataset = dataset.repeat()\n",
    "    dataset = dataset.map(partial(preprocess, augmentation=augmentation), num_parallel_calls=n_parse_threads)\n",
    "    return dataset.batch(batch_size).prefetch(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "0cc1bbca",
   "metadata": {},
   "outputs": [],
   "source": [
    "n_readers = 5\n",
    "shuffle_buffer_size = 400\n",
    "n_read_threads = None\n",
    "n_parse_threads = 5\n",
    "batch_size = 32\n",
    "\n",
    "train_set = read_csv_pipeline(train_paths, n_readers, \n",
    "                              n_repeat, shuffle_buffer_size, \n",
    "                              n_read_threads, n_parse_threads, \n",
    "                              batch_size, \n",
    "                              augmentation=True)\n",
    "\n",
    "valid_set = read_csv_pipeline(valid_paths, n_readers, \n",
    "                              n_repeat, shuffle_buffer_size, \n",
    "                              n_read_threads, n_parse_threads, \n",
    "                              batch_size, \n",
    "                              augmentation=False)\n",
    "\n",
    "test_set = read_csv_pipeline(test_paths, n_readers, \n",
    "                              n_repeat, shuffle_buffer_size, \n",
    "                              n_read_threads, n_parse_threads, \n",
    "                              batch_size, \n",
    "                              augmentation=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44c8b673",
   "metadata": {},
   "source": [
    "### Model Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "aca739d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ResidualLayer(keras.layers.Layer):\n",
    "    \n",
    "    def __init__(self, fm, strides=1, ksize=3, padding=\"same\", activation=\"relu\", **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "        self.fm = fm\n",
    "        self.ksize = ksize\n",
    "        self.strides = strides\n",
    "        self.padding = padding\n",
    "        self.activation = keras.activations.get(activation)\n",
    "        self.normalization = keras.layers.BatchNormalization()\n",
    "        self.mainc_layers = [keras.layers.Conv2D(self.fm, \n",
    "                                                 kernel_size=self.ksize, \n",
    "                                                 strides=self.strides, \n",
    "                                                 padding=self.padding, \n",
    "                                                 use_bias=False),\n",
    "                            self.normalization, \n",
    "                            self.activation,\n",
    "                            keras.layers.Conv2D(self.fm, \n",
    "                                                kernel_size=self.ksize, \n",
    "                                                strides=1, \n",
    "                                                padding=self.padding, \n",
    "                                                use_bias=False),\n",
    "                            self.normalization]\n",
    "        self.skipc_layers = []\n",
    "        if strides > 1:\n",
    "            self.skipc_layers = [keras.layers.Conv2D(self.fm, \n",
    "                                                     kernel_size=1, \n",
    "                                                     strides=self.strides, \n",
    "                                                     padding=self.padding,\n",
    "                                                     use_bias=False),\n",
    "                                self.normalization]\n",
    "    def get_config(self):\n",
    "        config = super().get_config()\n",
    "        config.update({\"fm\": self.fm,\n",
    "                        \"ksize\": self.ksize,\n",
    "                        \"strides\": self.strides,\n",
    "                        \"padding\": self.padding,\n",
    "                        \"activation\": self.activation\n",
    "                        })\n",
    "        return config\n",
    "    \n",
    "    def call(self, inputs):\n",
    "        z = inputs\n",
    "        for layer in self.mainc_layers:\n",
    "            z = layer(z)\n",
    "        skip_z = inputs\n",
    "        for layer in self.skipc_layers:\n",
    "            skip_z = layer(skip_z)\n",
    "        return self.activation(z+skip_z)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c803b55",
   "metadata": {},
   "outputs": [],
   "source": [
    "keras.backend.clear_session()\n",
    "np.random.seed(42)\n",
    "tf.random.set_seed(42)\n",
    "\n",
    "\n",
    "def cnn_mp_r3_de_v2():\n",
    "    \"\"\"\n",
    "    name ecodes the model architecture\n",
    "    cnn followed by max pooling repeated three times, followed by dense layer\n",
    "    changes: dense layer size increased to 128\n",
    "             he_normal initilizatin is implemented\n",
    "             optimizer is changed from nadam to momentum with decay rate\n",
    "    \"\"\"\n",
    "\n",
    "    model = keras.models.Sequential()\n",
    "    model.add(keras.layers.Conv2D(64, kernel_size=5, \n",
    "                                  strides=2, padding=\"same\", \n",
    "                                  use_bias=False, \n",
    "                                  input_shape=[dims[0], dims[0], channels],\n",
    "                                  kernel_initializer=\"he_normal\",\n",
    "                                  activation=\"relu\"))\n",
    "    model.add(keras.layers.MaxPool2D(pool_size=2))\n",
    "    model.add(keras.layers.Conv2D(128, kernel_size=3, \n",
    "                                  strides=1, padding=\"same\", \n",
    "                                  use_bias=False,\n",
    "                                  kernel_initializer=\"he_normal\",\n",
    "                                  activation=\"relu\"))\n",
    "    model.add(keras.layers.MaxPool2D(pool_size=2))\n",
    "    model.add(keras.layers.Conv2D(256, kernel_size=3, \n",
    "                                  strides=1, padding=\"same\", \n",
    "                                  use_bias=False, \n",
    "                                  kernel_initializer=\"he_normal\",\n",
    "                                  activation=\"relu\"))\n",
    "    model.add(keras.layers.MaxPool2D(pool_size=2))\n",
    "    model.add(keras.layers.Flatten())\n",
    "    model.add(keras.layers.Dropout(0.3))\n",
    "    model.add(keras.layers.Dense(128, activation=\"relu\"))\n",
    "    model.add(keras.layers.Dropout(0.3))\n",
    "    model.add(keras.layers.Dense(5, activation=\"softmax\"))\n",
    "    \n",
    "    optimizer = keras.optimizers.SGD(learning_rate=0.2, momentum=0.9, decay=0.01)\n",
    "    model.compile(loss=keras.losses.sparse_categorical_crossentropy, \n",
    "                  optimizer=optimizer, \n",
    "                  metrics=keras.metrics.sparse_categorical_accuracy)\n",
    "    \n",
    "    model_target = os.path.join(BASE_DIR, \"models\", \"cnn_mp_r3_de_v2.h5\")\n",
    "    checkpoint_cb = keras.callbacks.ModelCheckpoint(model_target, save_best_only=True)\n",
    "    early_stop_cb = keras.callbacks.EarlyStopping(patience=10)\n",
    "    callbacks = [checkpoint_cb, early_stop_cb]\n",
    "    \n",
    "    return model, callbacks"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
