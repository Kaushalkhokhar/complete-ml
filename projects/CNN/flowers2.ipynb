{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "70c7c4bf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "232"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import gc\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "883870bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib as mpl\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "fb38fc87",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow import keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ed745db3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of available GPU's:  0\n"
     ]
    }
   ],
   "source": [
    "print(\"Number of available GPU's: \", len(tf.config.experimental.list_physical_devices(\"GPU\")))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "3c1e4528",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'D:\\\\TheCompleteML\\\\projects'"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "if \"CNN\" in os.path.abspath(os.curdir): os.chdir(\"..\")\n",
    "BASE_DIR = os.path.abspath(os.curdir)\n",
    "BASE_DIR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "3e1d571d",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dir = os.path.join(BASE_DIR, \"datasets\", \"classification\", \"flowers\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "050b9368",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['D:\\\\TheCompleteML\\\\projects\\\\datasets\\\\classification\\\\flowers\\\\daisy',\n",
       " 'D:\\\\TheCompleteML\\\\projects\\\\datasets\\\\classification\\\\flowers\\\\dandelion',\n",
       " 'D:\\\\TheCompleteML\\\\projects\\\\datasets\\\\classification\\\\flowers\\\\rose',\n",
       " 'D:\\\\TheCompleteML\\\\projects\\\\datasets\\\\classification\\\\flowers\\\\sunflower',\n",
       " 'D:\\\\TheCompleteML\\\\projects\\\\datasets\\\\classification\\\\flowers\\\\tulip']"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_dirs = [os.path.join(data_dir, dir_) for dir_ in os.listdir(data_dir) if \"processed\" not in dir_]\n",
    "data_dirs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "062293b0",
   "metadata": {},
   "source": [
    "## Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "id": "e24d425d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "from skimage import io\n",
    "\n",
    "class loading_and_splitting:\n",
    "    \n",
    "    def __init__(self, data_dirs, dims, channels=3, target_dir=data_dir, split_count=10):\n",
    "        self.total_images = 0\n",
    "        self.minh = np.inf\n",
    "        self.minw = np.inf\n",
    "        self.dims = dims\n",
    "        self.channels = channels\n",
    "        self.target_dir = target_dir\n",
    "        self.split_count = split_count\n",
    "        self.data_dirs = data_dirs\n",
    "        self.class_map = {k:v.split(\"\\\\\")[-1] for k, v in enumerate(data_dirs)}\n",
    "        \n",
    "        self.header_list = [f\"x{i}\" for i in range(self.dims[0]*self.dims[1]*self.channels)] + [\"label\"]\n",
    "        self.sample_list = [random.sample(range(len(os.listdir(path))), \n",
    "                                          len(os.listdir(path))) for path in data_dirs]\n",
    "        for item in self.sample_list:\n",
    "            self.total_images += len(item)\n",
    "        self.generate_samples()\n",
    "        self.max_train_instance = len(self.train_seq)\n",
    "        self.max_valid_instance = len(self.valid_seq)\n",
    "        self.max_test_instance = len(self.test_seq)\n",
    "    \n",
    "    def generate_csvs(self):\n",
    "        header_list = [f\"x{i}\" for i in range(self.dims[0]*self.dims[1]*self.channels)] + [\"label\"]\n",
    "        for set_ in [\"train\", \"valid\", \"test\"]:\n",
    "            with open(os.path.join(self.target_dir, f\"{set_}.csv\"), \"w\") as f:\n",
    "                df = pd.DataFrame(list(), columns=header_list)\n",
    "                df.to_csv(f, index=False)\n",
    "                \n",
    "    def generate_samples(self):\n",
    "        self.sample_seq = random.sample(range(self.total_images), self.total_images)\n",
    "        self.train_seq = self.sample_seq[:int(len(self.sample_seq)*0.8)]\n",
    "        self.valid_seq = self.sample_seq[int(len(self.sample_seq)*0.8):int(len(self.sample_seq)*0.9)]\n",
    "        self.test_seq = self.sample_seq[int(len(self.sample_seq)*0.9):]\n",
    "    \n",
    "    def crop_image(self, image):\n",
    "        h, w, d = image.shape\n",
    "        if h >= self.minh and w >= self.minw:\n",
    "            image = image[int(h/2)-64:int(h/2)+64, \n",
    "                          int(w/2)-64:int(w/2)+64, \n",
    "                          :]\n",
    "            return image\n",
    "    \n",
    "    def crop_or_pad(self, image):\n",
    "        image = tf.image.resize_with_crop_or_pad(image, self.dims[0], self.dims[0])\n",
    "        return image.numpy()\n",
    "    \n",
    "    def shuffle_and_save(self):\n",
    "        empty = []\n",
    "        train = np.zeros((1, self.dims[0]*self.dims[1]*self.channels + 1))\n",
    "        valid = np.zeros((1, self.dims[0]*self.dims[1]*self.channels + 1))\n",
    "        test = np.zeros((1, self.dims[0]*self.dims[1]*self.channels + 1))\n",
    "        count = 0\n",
    "        while len(empty) != len(self.data_dirs):\n",
    "            sel_dir = np.random.randint(0, len(self.data_dirs))\n",
    "            if sel_dir in empty: continue\n",
    "            dir_ = self.data_dirs[sel_dir]\n",
    "            if not self.sample_list[sel_dir]:\n",
    "                empty.append(sel_dir)\n",
    "            else:\n",
    "                count += 1\n",
    "                print(f\"Processing: {count}\")\n",
    "                sel_image = self.sample_list[sel_dir].pop()\n",
    "                image = io.imread(os.path.join(dir_, os.listdir(dir_)[sel_image]))\n",
    "                \n",
    "                h, w, d = image.shape\n",
    "                if h < self.minh: self.minh = h\n",
    "                if w < self.minw: self.minw = w\n",
    "                if self.minh < self.dims[0]: self.minh = self.dims[0]\n",
    "                if self.minw < self.dims[1]: self.minw = self.dims[1]\n",
    "                \n",
    "                # image = self.crop_image(image)\n",
    "                image = self.crop_or_pad(image)\n",
    "                \n",
    "                if not isinstance(image, np.ndarray): continue\n",
    "                if sel_image in self.train_seq: \n",
    "                    train = np.append(train, np.append(image.flatten(), sel_dir).reshape(1,-1), axis=0)\n",
    "                elif sel_image in self.valid_seq: \n",
    "                    valid = np.append(valid, np.append(image.flatten(), sel_dir).reshape(1,-1), axis=0)\n",
    "                elif sel_image in self.test_seq: \n",
    "                    test = np.append(test, np.append(image.flatten(), sel_dir).reshape(1,-1), axis=0)\n",
    "        \n",
    "        train = train[1:, :]\n",
    "        valid = valid[1:, :]\n",
    "        test = test[1:, :]\n",
    "        \n",
    "        for prefix, arr in zip([\"train\", \"valid\", \"test\"], [train, valid, test]):\n",
    "            self.split_and_save(arr, os.path.join(self.target_dir, \"processed\", prefix), prefix)\n",
    "        \n",
    "    def split_and_save(self, arr, target_dir, prefix):\n",
    "        os.makedirs(target_dir, exist_ok=True)\n",
    "        for i in range(self.split_count):\n",
    "            df = pd.DataFrame(arr[i*int(arr.shape[0]//self.split_count):(i+1)*int(arr.shape[0]//self.split_count), :], \n",
    "                             columns=self.header_list)\n",
    "            df.to_csv(os.path.join(target_dir, \"{}_{}.csv\".format(prefix, i+1)), index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "id": "1c7dbe96",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{0: 'daisy', 1: 'dandelion', 2: 'rose', 3: 'sunflower', 4: 'tulip'}"
      ]
     },
     "execution_count": 98,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dims = (150, 150)\n",
    "channels = 3\n",
    "n_features = dims[0] * dims[1] * channels\n",
    "split_count = 10\n",
    "ls = loading_and_splitting(data_dirs=data_dirs, dims=dims, channels=channels, target_dir=data_dir, split_count=split_count)\n",
    "class_map = ls.class_map\n",
    "max_train_instance = (ls.max_train_instance//split_count)*split_count\n",
    "max_valid_instance = (ls.max_valid_instance//split_count)*split_count\n",
    "max_test_instance = (ls.max_test_instance//split_count)*split_count\n",
    "class_map\n",
    "# ls.shuffle_and_save()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "id": "eb4a2833",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(3450, 430, 430)"
      ]
     },
     "execution_count": 99,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "max_train_instance, max_valid_instance, max_test_instance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "50371adf",
   "metadata": {},
   "outputs": [],
   "source": [
    "set_dir = os.path.join(data_dir, \"processed\")\n",
    "train_paths = [f\"{os.path.join(set_dir, 'train')}\\\\{item}\" for item in os.listdir(os.path.join(set_dir, \"train\"))]\n",
    "valid_paths = [f\"{os.path.join(set_dir, 'valid')}\\\\{item}\" for item in os.listdir(os.path.join(set_dir, \"valid\"))]\n",
    "test_paths = [f\"{os.path.join(set_dir, 'test')}\\\\{item}\" for item in os.listdir(os.path.join(set_dir, \"test\"))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "id": "5a00ff92",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_augmentation = tf.keras.Sequential([keras.layers.RandomFlip(\"horizontal_and_vertical\"), \n",
    "                                         keras.layers.RandomRotation(0.2),\n",
    "                                         keras.layers.RandomContrast(0.5),\n",
    "                                         keras.layers.RandomZoom((-0.3, 0.3), (-0.3, 0.3))\n",
    "                                         ])\n",
    "\n",
    "def preprocess_tran_ler(line, \n",
    "                        augmentation=False, \n",
    "                        resize=(224, 224)):\n",
    "    defs = [tf.constant([], dtype = tf.float32)] * (n_features + 1)\n",
    "    xy = tf.io.decode_csv(line, record_defaults=defs)\n",
    "    X = tf.stack(xy[:-1])\n",
    "    y = tf.stack(xy[-1:])\n",
    "    \n",
    "    # prcessing steps\n",
    "    X = tf.reshape(X, [dims[0], dims[1], channels])\n",
    "    X = tf.image.resize(X, size=resize)\n",
    "    if augmentation:\n",
    "        X = data_augmentation(X)\n",
    "        X = tf.image.rot90(X)\n",
    "        X = tf.image.random_brightness(X, 0.2)\n",
    "    X = keras.applications.resnet50.preprocess_input(X)\n",
    "    return X, y\n",
    "\n",
    "\n",
    "def preprocess_test(X):\n",
    "    # prcessing steps\n",
    "    \n",
    "    X = tf.image.resize_with_crop_or_pad(X, 150 ,150)\n",
    "    X = data_augmentation(X)\n",
    "    X = tf.image.rot90(X)\n",
    "    X = tf.image.random_brightness(X, 0.2)\n",
    "    X = tf.divide(X, 255)\n",
    "    \n",
    "    return X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "id": "bd75f226",
   "metadata": {},
   "outputs": [],
   "source": [
    "from functools import partial\n",
    "\n",
    "def read_csv_pipeline(paths, \n",
    "                      n_readers, \n",
    "                      shuffle_buffer_size, \n",
    "                      n_read_threds, \n",
    "                      n_parse_threads, \n",
    "                      batch_size, \n",
    "                      augmentation=False,\n",
    "                      resize=(224, 224)):\n",
    "    \n",
    "    filepaths = tf.data.Dataset.list_files(paths, seed=42)\n",
    "    dataset = filepaths.interleave(lambda filepath: tf.data.TextLineDataset(filepath).skip(1), cycle_length=n_readers)\n",
    "    dataset = dataset.shuffle(shuffle_buffer_size)\n",
    "    if augmentation: dataset = dataset.repeat()\n",
    "    dataset = dataset.map(partial(preprocess_tran_ler, augmentation=augmentation, resize=resize), num_parallel_calls=n_parse_threads)\n",
    "    return dataset.batch(batch_size).prefetch(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "id": "c8b086a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "n_readers = 5\n",
    "shuffle_buffer_size = 400\n",
    "n_read_threads = None\n",
    "n_parse_threads = 5\n",
    "batch_size = 32\n",
    "resize = (150, 150)\n",
    "\n",
    "train_set = read_csv_pipeline(train_paths, \n",
    "                              n_readers, \n",
    "                              shuffle_buffer_size, \n",
    "                              n_read_threads, \n",
    "                              n_parse_threads, \n",
    "                              batch_size, \n",
    "                              augmentation=True,\n",
    "                              resize=resize)\n",
    "\n",
    "valid_set = read_csv_pipeline(valid_paths, \n",
    "                              n_readers, \n",
    "                              shuffle_buffer_size, \n",
    "                              n_read_threads, \n",
    "                              n_parse_threads, \n",
    "                              batch_size, \n",
    "                              augmentation=False,\n",
    "                              resize=resize)\n",
    "\n",
    "test_set = read_csv_pipeline(test_paths, \n",
    "                             n_readers, \n",
    "                             shuffle_buffer_size, \n",
    "                             n_read_threads, \n",
    "                             n_parse_threads, \n",
    "                             batch_size, \n",
    "                             augmentation=False,\n",
    "                             resize=resize)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5db29415",
   "metadata": {},
   "source": [
    "### Model Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "3d28334f",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ResidualLayer(keras.layers.Layer):\n",
    "    \n",
    "    def __init__(self, fm, strides=1, ksize=3, padding=\"same\", activation=\"relu\", **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "        self.fm = fm\n",
    "        self.ksize = ksize\n",
    "        self.strides = strides\n",
    "        self.padding = padding\n",
    "        self.activation = keras.activations.get(activation)\n",
    "        self.normalization = keras.layers.BatchNormalization()\n",
    "        self.mainc_layers = [keras.layers.Conv2D(self.fm, \n",
    "                                                 kernel_size=self.ksize, \n",
    "                                                 strides=self.strides, \n",
    "                                                 padding=self.padding, \n",
    "                                                 use_bias=False),\n",
    "                            self.normalization, \n",
    "                            self.activation,\n",
    "                            keras.layers.Conv2D(self.fm, \n",
    "                                                kernel_size=self.ksize, \n",
    "                                                strides=1, \n",
    "                                                padding=self.padding, \n",
    "                                                use_bias=False),\n",
    "                            self.normalization]\n",
    "        self.skipc_layers = []\n",
    "        if strides > 1:\n",
    "            self.skipc_layers = [keras.layers.Conv2D(self.fm, \n",
    "                                                     kernel_size=1, \n",
    "                                                     strides=self.strides, \n",
    "                                                     padding=self.padding,\n",
    "                                                     use_bias=False),\n",
    "                                self.normalization]\n",
    "    def get_config(self):\n",
    "        config = super().get_config()\n",
    "        config.update({\"fm\": self.fm,\n",
    "                        \"ksize\": self.ksize,\n",
    "                        \"strides\": self.strides,\n",
    "                        \"padding\": self.padding,\n",
    "                        \"activation\": self.activation\n",
    "                        })\n",
    "        return config\n",
    "    \n",
    "    def call(self, inputs):\n",
    "        z = inputs\n",
    "        for layer in self.mainc_layers:\n",
    "            z = layer(z)\n",
    "        skip_z = inputs\n",
    "        for layer in self.skipc_layers:\n",
    "            skip_z = layer(skip_z)\n",
    "        return self.activation(z+skip_z)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "4b008502",
   "metadata": {},
   "outputs": [],
   "source": [
    "keras.backend.clear_session()\n",
    "np.random.seed(42)\n",
    "tf.random.set_seed(42)\n",
    "\n",
    "\n",
    "def cnn_mp_r3_de_v2():\n",
    "    \"\"\"\n",
    "    name ecodes the model architecture\n",
    "    cnn followed by max pooling repeated three times, followed by dense layer\n",
    "    changes: dense layer size increased to 128\n",
    "             he_normal initilizatin is implemented\n",
    "             optimizer is changed from nadam to momentum with decay rate\n",
    "    \"\"\"\n",
    "\n",
    "    model = keras.models.Sequential()\n",
    "    model.add(keras.layers.Conv2D(64, kernel_size=5, \n",
    "                                  strides=2, padding=\"same\", \n",
    "                                  use_bias=False, \n",
    "                                  input_shape=[dims[0], dims[0], channels],\n",
    "                                  kernel_initializer=\"he_normal\",\n",
    "                                  activation=\"relu\"))\n",
    "    model.add(keras.layers.MaxPool2D(pool_size=2))\n",
    "    model.add(keras.layers.Conv2D(128, kernel_size=3, \n",
    "                                  strides=1, padding=\"same\", \n",
    "                                  use_bias=False,\n",
    "                                  kernel_initializer=\"he_normal\",\n",
    "                                  activation=\"relu\"))\n",
    "    model.add(keras.layers.MaxPool2D(pool_size=2))\n",
    "    model.add(keras.layers.Conv2D(256, kernel_size=3, \n",
    "                                  strides=1, padding=\"same\", \n",
    "                                  use_bias=False,\n",
    "                                  kernel_initializer=\"he_normal\",\n",
    "                                  activation=\"relu\"))\n",
    "    model.add(keras.layers.MaxPool2D(pool_size=2))\n",
    "    model.add(keras.layers.Flatten())\n",
    "    model.add(keras.layers.Dropout(0.3))\n",
    "    model.add(keras.layers.Dense(128, activation=\"relu\"))\n",
    "    model.add(keras.layers.Dropout(0.3))\n",
    "    model.add(keras.layers.Dense(5, activation=\"softmax\"))\n",
    "    \n",
    "    optimizer = keras.optimizers.SGD(learning_rate=0.2, momentum=0.9, decay=0.01)\n",
    "    model.compile(loss=keras.losses.sparse_categorical_crossentropy, \n",
    "                  optimizer=optimizer, \n",
    "                  metrics=keras.metrics.sparse_categorical_accuracy)\n",
    "    \n",
    "    \n",
    "    model_target = os.path.join(BASE_DIR, \"models\", \"cnn_mp_r3_de_v2.h5\")\n",
    "    checkpoint_cb = keras.callbacks.ModelCheckpoint(model_target, save_best_only=True)\n",
    "    early_stop_cb = keras.callbacks.EarlyStopping(patience=10)\n",
    "    callbacks = [checkpoint_cb, early_stop_cb]\n",
    "    \n",
    "    return model, callbacks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "09011346",
   "metadata": {},
   "outputs": [],
   "source": [
    "model, callbacks = cnn_mp_r3_de_v2()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "aa1c2b0c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "97/97 [==============================] - 134s 1s/step - loss: nan - sparse_categorical_accuracy: 0.1814 - val_loss: nan - val_sparse_categorical_accuracy: 0.1719\n",
      "Epoch 2/5\n",
      "97/97 [==============================] - 130s 1s/step - loss: nan - sparse_categorical_accuracy: 0.1843 - val_loss: nan - val_sparse_categorical_accuracy: 0.1771\n",
      "Epoch 3/5\n",
      "97/97 [==============================] - 130s 1s/step - loss: nan - sparse_categorical_accuracy: 0.1775 - val_loss: nan - val_sparse_categorical_accuracy: 0.1719\n",
      "Epoch 4/5\n",
      "97/97 [==============================] - 132s 1s/step - loss: nan - sparse_categorical_accuracy: 0.1611 - val_loss: nan - val_sparse_categorical_accuracy: 0.1667\n",
      "Epoch 5/5\n",
      "97/97 [==============================] - 130s 1s/step - loss: nan - sparse_categorical_accuracy: 0.1817 - val_loss: nan - val_sparse_categorical_accuracy: 0.1693\n"
     ]
    }
   ],
   "source": [
    "history = model.fit(train_set, \n",
    "                    epochs=5, \n",
    "                    steps_per_epoch=int(0.9 * max_train_instance / batch_size),\n",
    "                    validation_data=valid_set, \n",
    "                    validation_steps=int(0.9 * max_valid_instance / batch_size),\n",
    "                    callbacks=callbacks)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1b04270",
   "metadata": {},
   "source": [
    "### Training with different optimizers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "fa03a5cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "keras.backend.clear_session()\n",
    "np.random.seed(42)\n",
    "tf.random.set_seed(42)\n",
    "\n",
    "\n",
    "def cnn_mp_r3_de_v2():\n",
    "   \n",
    "    model = keras.models.Sequential()\n",
    "    model.add(keras.layers.Conv2D(64, kernel_size=5, \n",
    "                                  strides=2, padding=\"same\", \n",
    "                                  use_bias=False, \n",
    "                                  input_shape=[dims[0], dims[0], channels],\n",
    "                                  activation=\"relu\"))\n",
    "    model.add(keras.layers.MaxPool2D(pool_size=2))\n",
    "    model.add(keras.layers.Conv2D(128, kernel_size=3, \n",
    "                                  strides=1, padding=\"same\", \n",
    "                                  use_bias=False,\n",
    "                                  activation=\"relu\"))\n",
    "    model.add(keras.layers.MaxPool2D(pool_size=2))\n",
    "    model.add(keras.layers.Conv2D(256, kernel_size=3, \n",
    "                                  strides=1, padding=\"same\", \n",
    "                                  use_bias=False,\n",
    "                                  activation=\"relu\"))\n",
    "    model.add(keras.layers.MaxPool2D(pool_size=2))\n",
    "    model.add(keras.layers.Flatten())\n",
    "    model.add(keras.layers.Dropout(0.3))\n",
    "    model.add(keras.layers.Dense(64, activation=\"relu\"))\n",
    "    model.add(keras.layers.Dropout(0.3))\n",
    "    model.add(keras.layers.Dense(5, activation=\"softmax\"))\n",
    "    \n",
    "#     optimizer = keras.optimizers.SGD(learning_rate=0.2, momentum=0.9, decay=0.01)\n",
    "    model.compile(loss=keras.losses.sparse_categorical_crossentropy, \n",
    "                  optimizer=\"nadam\", \n",
    "                  metrics=keras.metrics.sparse_categorical_accuracy)\n",
    "    \n",
    "    \n",
    "    model_target = os.path.join(BASE_DIR, \"models\", \"cnn_mp_r3_de_v2.h5\")\n",
    "    checkpoint_cb = keras.callbacks.ModelCheckpoint(model_target, save_best_only=True)\n",
    "    early_stop_cb = keras.callbacks.EarlyStopping(patience=10)\n",
    "    callbacks = [checkpoint_cb, early_stop_cb]\n",
    "    \n",
    "    return model, callbacks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "cac14ac5",
   "metadata": {},
   "outputs": [],
   "source": [
    "model, callbacks = cnn_mp_r3_de_v2()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "1cd4fc2c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "97/97 [==============================] - 138s 1s/step - loss: 1.4514 - sparse_categorical_accuracy: 0.3553 - val_loss: 1.2857 - val_sparse_categorical_accuracy: 0.3854\n",
      "Epoch 2/5\n",
      "97/97 [==============================] - 137s 1s/step - loss: 1.3074 - sparse_categorical_accuracy: 0.4446 - val_loss: 1.3003 - val_sparse_categorical_accuracy: 0.4089\n",
      "Epoch 3/5\n",
      "97/97 [==============================] - 134s 1s/step - loss: 1.2394 - sparse_categorical_accuracy: 0.4726 - val_loss: 1.1672 - val_sparse_categorical_accuracy: 0.5469\n",
      "Epoch 4/5\n",
      "97/97 [==============================] - 133s 1s/step - loss: 1.2192 - sparse_categorical_accuracy: 0.5200 - val_loss: 1.1271 - val_sparse_categorical_accuracy: 0.5807\n",
      "Epoch 5/5\n",
      "97/97 [==============================] - 132s 1s/step - loss: 1.1396 - sparse_categorical_accuracy: 0.5467 - val_loss: 1.0547 - val_sparse_categorical_accuracy: 0.5781\n"
     ]
    }
   ],
   "source": [
    "history = model.fit(train_set, \n",
    "                    epochs=5, \n",
    "                    steps_per_epoch=int(0.9 * max_train_instance / batch_size),\n",
    "                    validation_data=valid_set, \n",
    "                    validation_steps=int(0.9 * max_valid_instance / batch_size),\n",
    "                    callbacks=callbacks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "dde21c32",
   "metadata": {},
   "outputs": [],
   "source": [
    " keras.backend.clear_session()\n",
    "np.random.seed(42)\n",
    "tf.random.set_seed(42)\n",
    "\n",
    "\n",
    "def cnn_mp_r3_de_v2():\n",
    "  \n",
    "\n",
    "    model = keras.models.Sequential()\n",
    "    model.add(keras.layers.Conv2D(64, kernel_size=5, \n",
    "                                  strides=2, padding=\"same\", \n",
    "                                  use_bias=False, \n",
    "                                  input_shape=[dims[0], dims[0], channels],\n",
    "                                  activation=\"relu\"))\n",
    "    model.add(keras.layers.MaxPool2D(pool_size=2))\n",
    "    model.add(keras.layers.Conv2D(128, kernel_size=3, \n",
    "                                  strides=1, padding=\"same\", \n",
    "                                  use_bias=False,\n",
    "                                  activation=\"relu\"))\n",
    "    model.add(keras.layers.MaxPool2D(pool_size=2))\n",
    "    model.add(keras.layers.Conv2D(256, kernel_size=3, \n",
    "                                  strides=1, padding=\"same\", \n",
    "                                  use_bias=False,\n",
    "                                  activation=\"relu\"))\n",
    "    model.add(keras.layers.MaxPool2D(pool_size=2))\n",
    "    model.add(keras.layers.Flatten())\n",
    "    model.add(keras.layers.Dropout(0.3))\n",
    "    model.add(keras.layers.Dense(64, activation=\"relu\"))\n",
    "    model.add(keras.layers.Dropout(0.3))\n",
    "    model.add(keras.layers.Dense(5, activation=\"softmax\"))\n",
    "    \n",
    "    optimizer = keras.optimizers.SGD(learning_rate=0.2, momentum=0.9, decay=0.01)\n",
    "    model.compile(loss=keras.losses.sparse_categorical_crossentropy, \n",
    "                  optimizer=optimizer, \n",
    "                  metrics=keras.metrics.sparse_categorical_accuracy)\n",
    "    \n",
    "    \n",
    "    model_target = os.path.join(BASE_DIR, \"models\", \"cnn_mp_r3_de_v2.h5\")\n",
    "    checkpoint_cb = keras.callbacks.ModelCheckpoint(model_target, save_best_only=True)\n",
    "    early_stop_cb = keras.callbacks.EarlyStopping(patience=10)\n",
    "    callbacks = [checkpoint_cb, early_stop_cb]\n",
    "    \n",
    "    return model, callbacks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "33881432",
   "metadata": {},
   "outputs": [],
   "source": [
    "model, callbacks = cnn_mp_r3_de_v2()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "e6833eca",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "97/97 [==============================] - 136s 1s/step - loss: 1.6377 - sparse_categorical_accuracy: 0.2574 - val_loss: 1.6638 - val_sparse_categorical_accuracy: 0.1745\n",
      "Epoch 2/5\n",
      "97/97 [==============================] - 130s 1s/step - loss: 1.6141 - sparse_categorical_accuracy: 0.2378 - val_loss: 1.6102 - val_sparse_categorical_accuracy: 0.2474\n",
      "Epoch 3/5\n",
      "97/97 [==============================] - 134s 1s/step - loss: 1.6052 - sparse_categorical_accuracy: 0.2239 - val_loss: 1.5973 - val_sparse_categorical_accuracy: 0.2474\n",
      "Epoch 4/5\n",
      "97/97 [==============================] - 133s 1s/step - loss: 1.5893 - sparse_categorical_accuracy: 0.2610 - val_loss: 1.5938 - val_sparse_categorical_accuracy: 0.2500\n",
      "Epoch 5/5\n",
      "54/97 [===============>..............] - ETA: 43s - loss: 1.6123 - sparse_categorical_accuracy: 0.1921"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_4976/1190883073.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m history = model.fit(train_set, \n\u001b[0m\u001b[0;32m      2\u001b[0m                     \u001b[0mepochs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m5\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m                     \u001b[0msteps_per_epoch\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m0.9\u001b[0m \u001b[1;33m*\u001b[0m \u001b[0mmax_train_instance\u001b[0m \u001b[1;33m/\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m                     \u001b[0mvalidation_data\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mvalid_set\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m                     \u001b[0mvalidation_steps\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m0.9\u001b[0m \u001b[1;33m*\u001b[0m \u001b[0mmax_valid_instance\u001b[0m \u001b[1;33m/\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\TheCompleteML\\env\\lib\\site-packages\\keras\\utils\\traceback_utils.py\u001b[0m in \u001b[0;36merror_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     62\u001b[0m     \u001b[0mfiltered_tb\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     63\u001b[0m     \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 64\u001b[1;33m       \u001b[1;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     65\u001b[0m     \u001b[1;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m  \u001b[1;31m# pylint: disable=broad-except\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     66\u001b[0m       \u001b[0mfiltered_tb\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0m_process_traceback_frames\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0me\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__traceback__\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\TheCompleteML\\env\\lib\\site-packages\\keras\\engine\\training.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[0;32m   1214\u001b[0m                 _r=1):\n\u001b[0;32m   1215\u001b[0m               \u001b[0mcallbacks\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mon_train_batch_begin\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mstep\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1216\u001b[1;33m               \u001b[0mtmp_logs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtrain_function\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0miterator\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1217\u001b[0m               \u001b[1;32mif\u001b[0m \u001b[0mdata_handler\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshould_sync\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1218\u001b[0m                 \u001b[0mcontext\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0masync_wait\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\TheCompleteML\\env\\lib\\site-packages\\tensorflow\\python\\util\\traceback_utils.py\u001b[0m in \u001b[0;36merror_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    148\u001b[0m     \u001b[0mfiltered_tb\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    149\u001b[0m     \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 150\u001b[1;33m       \u001b[1;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    151\u001b[0m     \u001b[1;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    152\u001b[0m       \u001b[0mfiltered_tb\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0m_process_traceback_frames\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0me\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__traceback__\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\TheCompleteML\\env\\lib\\site-packages\\tensorflow\\python\\eager\\def_function.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *args, **kwds)\u001b[0m\n\u001b[0;32m    908\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    909\u001b[0m       \u001b[1;32mwith\u001b[0m \u001b[0mOptionalXlaContext\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_jit_compile\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 910\u001b[1;33m         \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    911\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    912\u001b[0m       \u001b[0mnew_tracing_count\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mexperimental_get_tracing_count\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\TheCompleteML\\env\\lib\\site-packages\\tensorflow\\python\\eager\\def_function.py\u001b[0m in \u001b[0;36m_call\u001b[1;34m(self, *args, **kwds)\u001b[0m\n\u001b[0;32m    940\u001b[0m       \u001b[1;31m# In this case we have created variables on the first call, so we run the\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    941\u001b[0m       \u001b[1;31m# defunned version which is guaranteed to never create variables.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 942\u001b[1;33m       \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_stateless_fn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m  \u001b[1;31m# pylint: disable=not-callable\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    943\u001b[0m     \u001b[1;32melif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_stateful_fn\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    944\u001b[0m       \u001b[1;31m# Release the lock early so that multiple threads can perform the call\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\TheCompleteML\\env\\lib\\site-packages\\tensorflow\\python\\eager\\function.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   3128\u001b[0m       (graph_function,\n\u001b[0;32m   3129\u001b[0m        filtered_flat_args) = self._maybe_define_function(args, kwargs)\n\u001b[1;32m-> 3130\u001b[1;33m     return graph_function._call_flat(\n\u001b[0m\u001b[0;32m   3131\u001b[0m         filtered_flat_args, captured_inputs=graph_function.captured_inputs)  # pylint: disable=protected-access\n\u001b[0;32m   3132\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\TheCompleteML\\env\\lib\\site-packages\\tensorflow\\python\\eager\\function.py\u001b[0m in \u001b[0;36m_call_flat\u001b[1;34m(self, args, captured_inputs, cancellation_manager)\u001b[0m\n\u001b[0;32m   1957\u001b[0m         and executing_eagerly):\n\u001b[0;32m   1958\u001b[0m       \u001b[1;31m# No tape is watching; skip to running the function.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1959\u001b[1;33m       return self._build_call_outputs(self._inference_function.call(\n\u001b[0m\u001b[0;32m   1960\u001b[0m           ctx, args, cancellation_manager=cancellation_manager))\n\u001b[0;32m   1961\u001b[0m     forward_backward = self._select_forward_and_backward_functions(\n",
      "\u001b[1;32mD:\\TheCompleteML\\env\\lib\\site-packages\\tensorflow\\python\\eager\\function.py\u001b[0m in \u001b[0;36mcall\u001b[1;34m(self, ctx, args, cancellation_manager)\u001b[0m\n\u001b[0;32m    596\u001b[0m       \u001b[1;32mwith\u001b[0m \u001b[0m_InterpolateFunctionError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    597\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mcancellation_manager\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 598\u001b[1;33m           outputs = execute.execute(\n\u001b[0m\u001b[0;32m    599\u001b[0m               \u001b[0mstr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msignature\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mname\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    600\u001b[0m               \u001b[0mnum_outputs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_num_outputs\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\TheCompleteML\\env\\lib\\site-packages\\tensorflow\\python\\eager\\execute.py\u001b[0m in \u001b[0;36mquick_execute\u001b[1;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[0;32m     56\u001b[0m   \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     57\u001b[0m     \u001b[0mctx\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mensure_initialized\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 58\u001b[1;33m     tensors = pywrap_tfe.TFE_Py_Execute(ctx._handle, device_name, op_name,\n\u001b[0m\u001b[0;32m     59\u001b[0m                                         inputs, attrs, num_outputs)\n\u001b[0;32m     60\u001b[0m   \u001b[1;32mexcept\u001b[0m \u001b[0mcore\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_NotOkStatusException\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "history = model.fit(train_set, \n",
    "                    epochs=5, \n",
    "                    steps_per_epoch=int(0.9 * max_train_instance / batch_size),\n",
    "                    validation_data=valid_set, \n",
    "                    validation_steps=int(0.9 * max_valid_instance / batch_size),\n",
    "                    callbacks=callbacks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "54d8f5d1",
   "metadata": {},
   "outputs": [],
   "source": [
    " keras.backend.clear_session()\n",
    "np.random.seed(42)\n",
    "tf.random.set_seed(42)\n",
    "\n",
    "\n",
    "def cnn_mp_r3_de_v2():\n",
    "    \n",
    "    model = keras.models.Sequential()\n",
    "    model.add(keras.layers.Conv2D(64, kernel_size=5, \n",
    "                                  strides=2, padding=\"same\", \n",
    "                                  use_bias=False, \n",
    "                                  input_shape=[dims[0], dims[0], channels],\n",
    "                                  activation=\"relu\"))\n",
    "    model.add(keras.layers.MaxPool2D(pool_size=2))\n",
    "    model.add(keras.layers.Conv2D(128, kernel_size=3, \n",
    "                                  strides=1, padding=\"same\", \n",
    "                                  use_bias=False,\n",
    "                                  activation=\"relu\"))\n",
    "    model.add(keras.layers.MaxPool2D(pool_size=2))\n",
    "    model.add(keras.layers.Conv2D(256, kernel_size=3, \n",
    "                                  strides=1, padding=\"same\", \n",
    "                                  use_bias=False,\n",
    "                                  activation=\"relu\"))\n",
    "    model.add(keras.layers.MaxPool2D(pool_size=2))\n",
    "    model.add(keras.layers.Flatten())\n",
    "    model.add(keras.layers.Dropout(0.3))\n",
    "    model.add(keras.layers.Dense(64, activation=\"relu\"))\n",
    "    model.add(keras.layers.Dropout(0.3))\n",
    "    model.add(keras.layers.Dense(5, activation=\"softmax\"))\n",
    "    \n",
    "    optimizer = keras.optimizers.SGD(learning_rate=0.01, momentum=0.9, decay=0.01)\n",
    "    model.compile(loss=keras.losses.sparse_categorical_crossentropy, \n",
    "                  optimizer=optimizer, \n",
    "                  metrics=keras.metrics.sparse_categorical_accuracy)\n",
    "    \n",
    "    \n",
    "    model_target = os.path.join(BASE_DIR, \"models\", \"cnn_mp_r3_de_v2.h5\")\n",
    "    checkpoint_cb = keras.callbacks.ModelCheckpoint(model_target, save_best_only=True)\n",
    "    early_stop_cb = keras.callbacks.EarlyStopping(patience=10)\n",
    "    callbacks = [checkpoint_cb, early_stop_cb]\n",
    "    \n",
    "    return model, callbacks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "b3d41665",
   "metadata": {},
   "outputs": [],
   "source": [
    "model, callbacks = cnn_mp_r3_de_v2()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "e6b89766",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "97/97 [==============================] - 136s 1s/step - loss: 1.4953 - sparse_categorical_accuracy: 0.3283 - val_loss: 1.3413 - val_sparse_categorical_accuracy: 0.3750\n",
      "Epoch 2/5\n",
      "97/97 [==============================] - 133s 1s/step - loss: 1.3042 - sparse_categorical_accuracy: 0.4288 - val_loss: 1.2589 - val_sparse_categorical_accuracy: 0.4635\n",
      "Epoch 3/5\n",
      "97/97 [==============================] - 131s 1s/step - loss: 1.2657 - sparse_categorical_accuracy: 0.4555 - val_loss: 1.1900 - val_sparse_categorical_accuracy: 0.4948\n",
      "Epoch 4/5\n",
      "97/97 [==============================] - 134s 1s/step - loss: 1.2398 - sparse_categorical_accuracy: 0.4771 - val_loss: 1.1691 - val_sparse_categorical_accuracy: 0.4896\n",
      "Epoch 5/5\n",
      "97/97 [==============================] - 131s 1s/step - loss: 1.2400 - sparse_categorical_accuracy: 0.4733 - val_loss: 1.1564 - val_sparse_categorical_accuracy: 0.5130\n"
     ]
    }
   ],
   "source": [
    "history = model.fit(train_set, \n",
    "                    epochs=5, \n",
    "                    steps_per_epoch=int(0.9 * max_train_instance / batch_size),\n",
    "                    validation_data=valid_set, \n",
    "                    validation_steps=int(0.9 * max_valid_instance / batch_size),\n",
    "                    callbacks=callbacks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "d78cdbeb",
   "metadata": {},
   "outputs": [],
   "source": [
    " keras.backend.clear_session()\n",
    "np.random.seed(42)\n",
    "tf.random.set_seed(42)\n",
    "\n",
    "\n",
    "def cnn_mp_r3_de_v2():\n",
    "  \n",
    "    model = keras.models.Sequential()\n",
    "    model.add(keras.layers.Conv2D(64, kernel_size=5, \n",
    "                                  strides=2, padding=\"same\", \n",
    "                                  use_bias=False, \n",
    "                                  input_shape=[dims[0], dims[0], channels],\n",
    "                                  activation=\"relu\"))\n",
    "    model.add(keras.layers.MaxPool2D(pool_size=2))\n",
    "    model.add(keras.layers.Conv2D(128, kernel_size=3, \n",
    "                                  strides=1, padding=\"same\", \n",
    "                                  use_bias=False,\n",
    "                                  activation=\"relu\"))\n",
    "    model.add(keras.layers.MaxPool2D(pool_size=2))\n",
    "    model.add(keras.layers.Conv2D(256, kernel_size=3, \n",
    "                                  strides=1, padding=\"same\", \n",
    "                                  use_bias=False,\n",
    "                                  activation=\"relu\"))\n",
    "    model.add(keras.layers.MaxPool2D(pool_size=2))\n",
    "    model.add(keras.layers.Flatten())\n",
    "    model.add(keras.layers.Dropout(0.3))\n",
    "    model.add(keras.layers.Dense(64, activation=\"relu\"))\n",
    "    model.add(keras.layers.Dropout(0.3))\n",
    "    model.add(keras.layers.Dense(5, activation=\"softmax\"))\n",
    "    \n",
    "    optimizer = keras.optimizers.SGD(learning_rate=0.01, momentum=0.9)\n",
    "    model.compile(loss=keras.losses.sparse_categorical_crossentropy, \n",
    "                  optimizer=optimizer, \n",
    "                  metrics=keras.metrics.sparse_categorical_accuracy)\n",
    "    \n",
    "    \n",
    "    model_target = os.path.join(BASE_DIR, \"models\", \"cnn_mp_r3_de_v2.h5\")\n",
    "    checkpoint_cb = keras.callbacks.ModelCheckpoint(model_target, save_best_only=True)\n",
    "    early_stop_cb = keras.callbacks.EarlyStopping(patience=10)\n",
    "    callbacks = [checkpoint_cb, early_stop_cb]\n",
    "    \n",
    "    return model, callbacks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "f88f25e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "model, callbacks = cnn_mp_r3_de_v2()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "c0a8dbb8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "97/97 [==============================] - 133s 1s/step - loss: 1.5109 - sparse_categorical_accuracy: 0.3070 - val_loss: 1.2921 - val_sparse_categorical_accuracy: 0.3828\n",
      "Epoch 2/5\n",
      "97/97 [==============================] - 128s 1s/step - loss: 1.3329 - sparse_categorical_accuracy: 0.4162 - val_loss: 1.2634 - val_sparse_categorical_accuracy: 0.4896\n",
      "Epoch 3/5\n",
      "97/97 [==============================] - 131s 1s/step - loss: 1.3028 - sparse_categorical_accuracy: 0.4265 - val_loss: 1.2135 - val_sparse_categorical_accuracy: 0.4453\n",
      "Epoch 4/5\n",
      "97/97 [==============================] - 130s 1s/step - loss: 1.2508 - sparse_categorical_accuracy: 0.4671 - val_loss: 1.1935 - val_sparse_categorical_accuracy: 0.4870\n",
      "Epoch 5/5\n",
      "97/97 [==============================] - 130s 1s/step - loss: 1.1902 - sparse_categorical_accuracy: 0.5061 - val_loss: 1.1198 - val_sparse_categorical_accuracy: 0.5312\n"
     ]
    }
   ],
   "source": [
    "history = model.fit(train_set, \n",
    "                    epochs=5, \n",
    "                    steps_per_epoch=int(0.9 * max_train_instance / batch_size),\n",
    "                    validation_data=valid_set, \n",
    "                    validation_steps=int(0.9 * max_valid_instance / batch_size),\n",
    "                    callbacks=callbacks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "ef5730b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "keras.backend.clear_session()\n",
    "np.random.seed(42)\n",
    "tf.random.set_seed(42)\n",
    "\n",
    "\n",
    "def cnn_mp_r3_de_v2():\n",
    "    \"\"\"\n",
    "    name ecodes the model architecture\n",
    "    cnn followed by max pooling repeated three times, followed by dense layer\n",
    "    changes: dense layer size increased to 128\n",
    "             he_normal initilizatin is implemented\n",
    "             optimizer is changed from nadam to momentum with decay rate\n",
    "    \"\"\"\n",
    "\n",
    "    model = keras.models.Sequential()\n",
    "    model.add(keras.layers.Conv2D(64, kernel_size=5, \n",
    "                                  strides=2, padding=\"same\", \n",
    "                                  use_bias=False, \n",
    "                                  input_shape=[dims[0], dims[0], channels],\n",
    "                                  activation=\"relu\"))\n",
    "    model.add(keras.layers.MaxPool2D(pool_size=2))\n",
    "    model.add(keras.layers.Conv2D(128, kernel_size=3, \n",
    "                                  strides=1, padding=\"same\", \n",
    "                                  use_bias=False,\n",
    "                                  activation=\"relu\"))\n",
    "    model.add(keras.layers.MaxPool2D(pool_size=2))\n",
    "    model.add(keras.layers.Conv2D(256, kernel_size=3, \n",
    "                                  strides=1, padding=\"same\", \n",
    "                                  use_bias=False,\n",
    "                                  activation=\"relu\"))\n",
    "    model.add(keras.layers.MaxPool2D(pool_size=2))\n",
    "    model.add(keras.layers.Flatten())\n",
    "    model.add(keras.layers.Dropout(0.3))\n",
    "    model.add(keras.layers.Dense(64, activation=\"relu\"))\n",
    "    model.add(keras.layers.Dropout(0.3))\n",
    "    model.add(keras.layers.Dense(5, activation=\"softmax\"))\n",
    "    \n",
    "    model.compile(loss=keras.losses.sparse_categorical_crossentropy, \n",
    "                  optimizer=\"adam\", \n",
    "                  metrics=keras.metrics.sparse_categorical_accuracy)\n",
    "    \n",
    "    \n",
    "    model_target = os.path.join(BASE_DIR, \"models\", \"cnn_mp_r3_de_v2.h5\")\n",
    "    checkpoint_cb = keras.callbacks.ModelCheckpoint(model_target, save_best_only=True)\n",
    "    early_stop_cb = keras.callbacks.EarlyStopping(patience=10)\n",
    "    callbacks = [checkpoint_cb, early_stop_cb]\n",
    "    \n",
    "    return model, callbacks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "fc365e38",
   "metadata": {},
   "outputs": [],
   "source": [
    "model, callbacks = cnn_mp_r3_de_v2()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "ae56280b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "97/97 [==============================] - 133s 1s/step - loss: 1.4417 - sparse_categorical_accuracy: 0.3531 - val_loss: 1.2625 - val_sparse_categorical_accuracy: 0.4245\n",
      "Epoch 2/5\n",
      "97/97 [==============================] - 129s 1s/step - loss: 1.2789 - sparse_categorical_accuracy: 0.4433 - val_loss: 1.1716 - val_sparse_categorical_accuracy: 0.4609\n",
      "Epoch 3/5\n",
      "97/97 [==============================] - 129s 1s/step - loss: 1.2445 - sparse_categorical_accuracy: 0.4768 - val_loss: 1.1711 - val_sparse_categorical_accuracy: 0.5052\n",
      "Epoch 4/5\n",
      "97/97 [==============================] - 132s 1s/step - loss: 1.2316 - sparse_categorical_accuracy: 0.4739 - val_loss: 1.1175 - val_sparse_categorical_accuracy: 0.5469\n",
      "Epoch 5/5\n",
      "97/97 [==============================] - 130s 1s/step - loss: 1.1931 - sparse_categorical_accuracy: 0.4987 - val_loss: 1.0944 - val_sparse_categorical_accuracy: 0.5443\n"
     ]
    }
   ],
   "source": [
    "history = model.fit(train_set, \n",
    "                    epochs=5, \n",
    "                    steps_per_epoch=int(0.9 * max_train_instance / batch_size),\n",
    "                    validation_data=valid_set, \n",
    "                    validation_steps=int(0.9 * max_valid_instance / batch_size),\n",
    "                    callbacks=callbacks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "87859f57",
   "metadata": {},
   "outputs": [],
   "source": [
    " keras.backend.clear_session()\n",
    "np.random.seed(42)\n",
    "tf.random.set_seed(42)\n",
    "\n",
    "\n",
    "def cnn_mp_r3_de_v2():\n",
    "    \"\"\"\n",
    "    name ecodes the model architecture\n",
    "    cnn followed by max pooling repeated three times, followed by dense layer\n",
    "    changes: dense layer size increased to 128\n",
    "             he_normal initilizatin is implemented\n",
    "             optimizer is changed from nadam to momentum with decay rate\n",
    "    \"\"\"\n",
    "\n",
    "    model = keras.models.Sequential()\n",
    "    model.add(keras.layers.Conv2D(64, kernel_size=5, \n",
    "                                  strides=2, padding=\"same\", \n",
    "                                  use_bias=False, \n",
    "                                  input_shape=[dims[0], dims[0], channels],\n",
    "                                  activation=\"relu\"))\n",
    "    model.add(keras.layers.MaxPool2D(pool_size=2))\n",
    "    model.add(keras.layers.Conv2D(128, kernel_size=3, \n",
    "                                  strides=1, padding=\"same\", \n",
    "                                  use_bias=False,\n",
    "                                  activation=\"relu\"))\n",
    "    model.add(keras.layers.MaxPool2D(pool_size=2))\n",
    "    model.add(keras.layers.Conv2D(256, kernel_size=3, \n",
    "                                  strides=1, padding=\"same\", \n",
    "                                  use_bias=False,\n",
    "                                  activation=\"relu\"))\n",
    "    model.add(keras.layers.MaxPool2D(pool_size=2))\n",
    "    model.add(keras.layers.Flatten())\n",
    "    model.add(keras.layers.Dropout(0.3))\n",
    "    model.add(keras.layers.Dense(64, activation=\"relu\"))\n",
    "    model.add(keras.layers.Dropout(0.3))\n",
    "    model.add(keras.layers.Dense(5, activation=\"softmax\"))\n",
    "    \n",
    "    optimizer = keras.optimizers.RMSprop(learning_rate=0.001, rho=0.9)\n",
    "    model.compile(loss=keras.losses.sparse_categorical_crossentropy, \n",
    "                  optimizer=optimizer, \n",
    "                  metrics=keras.metrics.sparse_categorical_accuracy)\n",
    "    \n",
    "    \n",
    "    model_target = os.path.join(BASE_DIR, \"models\", \"cnn_mp_r3_de_v2.h5\")\n",
    "    checkpoint_cb = keras.callbacks.ModelCheckpoint(model_target, save_best_only=True)\n",
    "    early_stop_cb = keras.callbacks.EarlyStopping(patience=10)\n",
    "    callbacks = [checkpoint_cb, early_stop_cb]\n",
    "    \n",
    "    return model, callbacks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "cdc98ff2",
   "metadata": {},
   "outputs": [],
   "source": [
    "model, callbacks = cnn_mp_r3_de_v2()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "b5878d82",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "97/97 [==============================] - 145s 1s/step - loss: 1.4900 - sparse_categorical_accuracy: 0.3438 - val_loss: 1.2772 - val_sparse_categorical_accuracy: 0.4219\n",
      "Epoch 2/5\n",
      "97/97 [==============================] - 139s 1s/step - loss: 1.3380 - sparse_categorical_accuracy: 0.4214 - val_loss: 1.2705 - val_sparse_categorical_accuracy: 0.3854\n",
      "Epoch 3/5\n",
      "97/97 [==============================] - 139s 1s/step - loss: 1.2860 - sparse_categorical_accuracy: 0.4681 - val_loss: 1.2057 - val_sparse_categorical_accuracy: 0.5208\n",
      "Epoch 4/5\n",
      "97/97 [==============================] - 135s 1s/step - loss: 1.2318 - sparse_categorical_accuracy: 0.4974 - val_loss: 1.1199 - val_sparse_categorical_accuracy: 0.5260\n",
      "Epoch 5/5\n",
      "97/97 [==============================] - 141s 1s/step - loss: 1.2226 - sparse_categorical_accuracy: 0.5084 - val_loss: 1.1106 - val_sparse_categorical_accuracy: 0.5755\n"
     ]
    }
   ],
   "source": [
    "history = model.fit(train_set, \n",
    "                    epochs=5, \n",
    "                    steps_per_epoch=int(0.9 * max_train_instance / batch_size),\n",
    "                    validation_data=valid_set, \n",
    "                    validation_steps=int(0.9 * max_valid_instance / batch_size),\n",
    "                    callbacks=callbacks)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6be49fe",
   "metadata": {},
   "source": [
    "##### Observations\n",
    "\n",
    "Above results clearly concludes that nadam and RMSprop optimizer outperforms all others. Next, we can choose nadam.RMSprop and experiments with some other variables with model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bebffbbe",
   "metadata": {},
   "source": [
    "## Different Model Architecture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "8ee8f085",
   "metadata": {},
   "outputs": [],
   "source": [
    "keras.backend.clear_session()\n",
    "np.random.seed(42)\n",
    "tf.random.set_seed(42)\n",
    "\n",
    "\n",
    "def cnn_mp_r4_de():\n",
    "    \"\"\"\n",
    "    name ecodes the model architecture\n",
    "    cnn followed by max pooling repeated three times, followed by dense layer\n",
    "    changes copare to cnn_mp_r3_de:\n",
    "            extra cnn with kenel size of 512\n",
    "            dense layer size increased to 128\n",
    "            he_normal initilizatin is implemented\n",
    "    \"\"\"\n",
    "\n",
    "    model = keras.models.Sequential()\n",
    "    model.add(keras.layers.Conv2D(64, kernel_size=5, \n",
    "                                  strides=2, padding=\"same\", \n",
    "                                  use_bias=False, \n",
    "                                  input_shape=[dims[0], dims[0], channels],\n",
    "                                  kernel_initializer=\"he_normal\",\n",
    "                                  activation=\"relu\"))\n",
    "    model.add(keras.layers.MaxPool2D(pool_size=2))\n",
    "    model.add(keras.layers.Conv2D(128, kernel_size=3, \n",
    "                                  strides=1, padding=\"same\", \n",
    "                                  use_bias=False,\n",
    "                                  kernel_initializer=\"he_normal\",\n",
    "                                  activation=\"relu\"))\n",
    "    model.add(keras.layers.MaxPool2D(pool_size=2))\n",
    "    model.add(keras.layers.Conv2D(256, kernel_size=3, \n",
    "                                  strides=1, padding=\"same\", \n",
    "                                  use_bias=False,\n",
    "                                  kernel_initializer=\"he_normal\",\n",
    "                                  activation=\"relu\"))\n",
    "    model.add(keras.layers.MaxPool2D(pool_size=2))\n",
    "    model.add(keras.layers.Conv2D(512, kernel_size=3, \n",
    "                                  strides=1, padding=\"same\", \n",
    "                                  use_bias=False,\n",
    "                                  kernel_initializer=\"he_normal\",\n",
    "                                  activation=\"relu\"))\n",
    "    model.add(keras.layers.MaxPool2D(pool_size=2))\n",
    "    model.add(keras.layers.Flatten())\n",
    "    model.add(keras.layers.Dropout(0.3))\n",
    "    model.add(keras.layers.Dense(128, activation=\"relu\"))\n",
    "    model.add(keras.layers.Dropout(0.3))\n",
    "    model.add(keras.layers.Dense(5, activation=\"softmax\"))\n",
    "    \n",
    "    model.compile(loss=keras.losses.sparse_categorical_crossentropy, \n",
    "                  optimizer=\"nadam\", \n",
    "                  metrics=keras.metrics.sparse_categorical_accuracy)\n",
    "        \n",
    "    model_target = os.path.join(BASE_DIR, \"models\", \"cnn_mp_r4_de.h5\")\n",
    "    checkpoint_cb = keras.callbacks.ModelCheckpoint(model_target, save_best_only=True)\n",
    "    early_stop_cb = keras.callbacks.EarlyStopping(patience=10)\n",
    "    callbacks = [checkpoint_cb, early_stop_cb]\n",
    "    \n",
    "    return model, callbacks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "c5a15e0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "model, callbacks = cnn_mp_r4_de()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "27cbe5ff",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "97/97 [==============================] - 155s 2s/step - loss: 1.6924 - sparse_categorical_accuracy: 0.3254 - val_loss: 1.3802 - val_sparse_categorical_accuracy: 0.3776\n",
      "Epoch 2/5\n",
      "97/97 [==============================] - 151s 2s/step - loss: 1.3186 - sparse_categorical_accuracy: 0.4166 - val_loss: 1.2118 - val_sparse_categorical_accuracy: 0.4401\n",
      "Epoch 3/5\n",
      "97/97 [==============================] - 156s 2s/step - loss: 1.2511 - sparse_categorical_accuracy: 0.4720 - val_loss: 1.1742 - val_sparse_categorical_accuracy: 0.5104\n",
      "Epoch 4/5\n",
      "97/97 [==============================] - 155s 2s/step - loss: 1.2143 - sparse_categorical_accuracy: 0.4932 - val_loss: 1.1436 - val_sparse_categorical_accuracy: 0.5052\n",
      "Epoch 5/5\n",
      "97/97 [==============================] - 150s 2s/step - loss: 1.1657 - sparse_categorical_accuracy: 0.5245 - val_loss: 1.0710 - val_sparse_categorical_accuracy: 0.5443\n"
     ]
    }
   ],
   "source": [
    "history = model.fit(train_set, \n",
    "                    epochs=5, \n",
    "                    steps_per_epoch=int(0.9 * max_train_instance / batch_size),\n",
    "                    validation_data=valid_set, \n",
    "                    validation_steps=int(0.9 * max_valid_instance / batch_size),\n",
    "                    callbacks=callbacks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "fa2b90d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "keras.backend.clear_session()\n",
    "np.random.seed(42)\n",
    "tf.random.set_seed(42)\n",
    "\n",
    "\n",
    "def cnn_mp_r3_de_v2():\n",
    "    \"\"\"\n",
    "    name ecodes the model architecture\n",
    "    cnn followed by max pooling repeated three times, followed by dense layer\n",
    "    changes:\n",
    "            dense layer size increased to 128\n",
    "            he_normal initilizatin is implemented\n",
    "    \"\"\"\n",
    "\n",
    "    model = keras.models.Sequential()\n",
    "    model.add(keras.layers.Conv2D(64, kernel_size=5, \n",
    "                                  strides=2, padding=\"same\", \n",
    "                                  use_bias=False, \n",
    "                                  input_shape=[dims[0], dims[0], channels],\n",
    "                                  kernel_initializer=\"he_normal\",\n",
    "                                  activation=\"relu\"))\n",
    "    model.add(keras.layers.MaxPool2D(pool_size=2))\n",
    "    model.add(keras.layers.Conv2D(128, kernel_size=3, \n",
    "                                  strides=1, padding=\"same\", \n",
    "                                  use_bias=False,\n",
    "                                  kernel_initializer=\"he_normal\",\n",
    "                                  activation=\"relu\"))\n",
    "    model.add(keras.layers.MaxPool2D(pool_size=2))\n",
    "    model.add(keras.layers.Conv2D(256, kernel_size=3, \n",
    "                                  strides=1, padding=\"same\", \n",
    "                                  use_bias=False,\n",
    "                                  kernel_initializer=\"he_normal\",\n",
    "                                  activation=\"relu\"))\n",
    "    model.add(keras.layers.MaxPool2D(pool_size=2))\n",
    "    model.add(keras.layers.Flatten())\n",
    "    model.add(keras.layers.Dropout(0.3))\n",
    "    model.add(keras.layers.Dense(128, activation=\"relu\"))\n",
    "    model.add(keras.layers.Dropout(0.3))\n",
    "    model.add(keras.layers.Dense(5, activation=\"softmax\"))\n",
    "    \n",
    "    model.compile(loss=keras.losses.sparse_categorical_crossentropy, \n",
    "                  optimizer=\"nadam\", \n",
    "                  metrics=keras.metrics.sparse_categorical_accuracy)\n",
    "        \n",
    "    model_target = os.path.join(BASE_DIR, \"models\", \"cnn_mp_r3_de_v2.h5\")\n",
    "    checkpoint_cb = keras.callbacks.ModelCheckpoint(model_target, save_best_only=True)\n",
    "    early_stop_cb = keras.callbacks.EarlyStopping(patience=10)\n",
    "    callbacks = [checkpoint_cb, early_stop_cb]\n",
    "    \n",
    "    return model, callbacks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "96267da3",
   "metadata": {},
   "outputs": [],
   "source": [
    "model, callbacks = cnn_mp_r3_de_v2()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "5d8ab4c0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "97/97 [==============================] - 144s 1s/step - loss: 1.7884 - sparse_categorical_accuracy: 0.3541 - val_loss: 1.2552 - val_sparse_categorical_accuracy: 0.4531\n",
      "Epoch 2/5\n",
      "97/97 [==============================] - 139s 1s/step - loss: 1.2972 - sparse_categorical_accuracy: 0.4443 - val_loss: 1.1778 - val_sparse_categorical_accuracy: 0.4948\n",
      "Epoch 3/5\n",
      "97/97 [==============================] - 138s 1s/step - loss: 1.2268 - sparse_categorical_accuracy: 0.5000 - val_loss: 1.1302 - val_sparse_categorical_accuracy: 0.5495\n",
      "Epoch 4/5\n",
      "97/97 [==============================] - 141s 1s/step - loss: 1.1826 - sparse_categorical_accuracy: 0.4971 - val_loss: 1.0885 - val_sparse_categorical_accuracy: 0.5521\n",
      "Epoch 5/5\n",
      "97/97 [==============================] - 143s 1s/step - loss: 1.1258 - sparse_categorical_accuracy: 0.5506 - val_loss: 1.0160 - val_sparse_categorical_accuracy: 0.6250\n"
     ]
    }
   ],
   "source": [
    "history = model.fit(train_set, \n",
    "                    epochs=5, \n",
    "                    steps_per_epoch=int(0.9 * max_train_instance / batch_size),\n",
    "                    validation_data=valid_set, \n",
    "                    validation_steps=int(0.9 * max_valid_instance / batch_size),\n",
    "                    callbacks=callbacks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "b4a2f789",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/25\n",
      "97/97 [==============================] - 147s 2s/step - loss: 1.7125 - sparse_categorical_accuracy: 0.3576 - val_loss: 1.2798 - val_sparse_categorical_accuracy: 0.4167\n",
      "Epoch 2/25\n",
      "97/97 [==============================] - 141s 1s/step - loss: 1.3085 - sparse_categorical_accuracy: 0.4391 - val_loss: 1.2109 - val_sparse_categorical_accuracy: 0.4740\n",
      "Epoch 3/25\n",
      "97/97 [==============================] - 141s 1s/step - loss: 1.2280 - sparse_categorical_accuracy: 0.4742 - val_loss: 1.2233 - val_sparse_categorical_accuracy: 0.5078\n",
      "Epoch 4/25\n",
      "97/97 [==============================] - 134s 1s/step - loss: 1.2010 - sparse_categorical_accuracy: 0.5064 - val_loss: 1.0990 - val_sparse_categorical_accuracy: 0.5677\n",
      "Epoch 5/25\n",
      "97/97 [==============================] - 136s 1s/step - loss: 1.1398 - sparse_categorical_accuracy: 0.5370 - val_loss: 1.0768 - val_sparse_categorical_accuracy: 0.5573\n",
      "Epoch 6/25\n",
      "97/97 [==============================] - 135s 1s/step - loss: 1.1281 - sparse_categorical_accuracy: 0.5522 - val_loss: 1.0613 - val_sparse_categorical_accuracy: 0.5729\n",
      "Epoch 7/25\n",
      "97/97 [==============================] - 137s 1s/step - loss: 1.0861 - sparse_categorical_accuracy: 0.5660 - val_loss: 1.0771 - val_sparse_categorical_accuracy: 0.5443\n",
      "Epoch 8/25\n",
      "97/97 [==============================] - 134s 1s/step - loss: 1.0461 - sparse_categorical_accuracy: 0.5889 - val_loss: 1.0003 - val_sparse_categorical_accuracy: 0.6172\n",
      "Epoch 9/25\n",
      "79/97 [=======================>......] - ETA: 17s - loss: 1.0235 - sparse_categorical_accuracy: 0.5989WARNING:tensorflow:Your input ran out of data; interrupting training. Make sure that your dataset or generator can generate at least `steps_per_epoch * epochs` batches (in this case, 2425 batches). You may need to use the repeat() function when building your dataset.\n",
      "97/97 [==============================] - 113s 1s/step - loss: 1.0235 - sparse_categorical_accuracy: 0.5989 - val_loss: 0.9657 - val_sparse_categorical_accuracy: 0.5911\n"
     ]
    }
   ],
   "source": [
    "history = model.fit(train_set, \n",
    "                    epochs=25, \n",
    "                    steps_per_epoch=int(0.9 * max_train_instance / batch_size),\n",
    "                    validation_data=valid_set, \n",
    "                    validation_steps=int(0.9 * max_valid_instance / batch_size),\n",
    "                    callbacks=callbacks)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3c4c0ae",
   "metadata": {},
   "source": [
    "##### Note\n",
    "\n",
    "Done necessary adjustement to max_instances and steps_per_epochs to supress the warning message"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c13d2254",
   "metadata": {},
   "source": [
    "### Experiment with Learning Rate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "id": "bd334dd6",
   "metadata": {},
   "outputs": [],
   "source": [
    "K = keras.backend\n",
    "\n",
    "class ExponentialLearningRate(keras.callbacks.Callback):\n",
    "    def __init__(self, factor):\n",
    "        self.factor = factor\n",
    "        self.rates = []\n",
    "        self.losses = []\n",
    "        \n",
    "    def on_batch_end(self, batch, logs):\n",
    "        lr = K.get_value(self.model.optimizer.learning_rate)\n",
    "        self.rates.append(lr)\n",
    "        self.losses.append(logs[\"loss\"])\n",
    "        K.set_value(self.model.optimizer.learning_rate, lr*self.factor)\n",
    "        \n",
    "def find_learnig_rate(model, train_set, epochs=1, batch_size=32, min_rate=1e-5, max_rate=10):\n",
    "    init_weights = model.get_weights()\n",
    "    init_lr = K.get_value(model.optimizer.learning_rate)\n",
    "    iterations = np.math.ceil(0.85 * max_train_instance / batch_size) * epochs\n",
    "    factor = np.exp(np.log(max_rate / min_rate) / iterations)\n",
    "    exp_lr = ExponentialLearningRate(factor)\n",
    "    history = model.fit(train_set, \n",
    "                        epochs=epochs, \n",
    "                        steps_per_epoch=int(0.85 * max_train_instance / batch_size),\n",
    "                        batch_size=batch_size, \n",
    "                        callbacks=[exp_lr])\n",
    "    model.set_weights(init_weights)\n",
    "    K.set_value(model.optimizer.learning_rate, init_lr)\n",
    "    return exp_lr.rates, exp_lr.losses\n",
    "\n",
    "def plot_rates_vs_losses(rates, losses):\n",
    "    plt.plot(rates, losses)\n",
    "    plt.gca().set_xscale('log')\n",
    "    plt.hlines(min(losses), min(rates), max(rates))\n",
    "    plt.axis([min(rates), max(rates), min(losses), (losses[0] + min(losses)) / 2])\n",
    "    plt.xlabel(\"Learning rate\")\n",
    "    plt.ylabel(\"Loss\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "90733766",
   "metadata": {},
   "outputs": [],
   "source": [
    "class OneCycleScheduler(keras.callbacks.Callback):\n",
    "    def __init__(self, iterations, max_rate, start_rate=None,\n",
    "                 last_iterations=None, last_rate=None):\n",
    "        self.iterations = iterations\n",
    "        self.max_rate = max_rate\n",
    "        self.start_rate = start_rate or max_rate / 10\n",
    "        self.last_iterations = last_iterations or iterations // 10 + 1\n",
    "        self.half_iteration = (iterations - self.last_iterations) // 2\n",
    "        self.last_rate = last_rate or self.start_rate / 1000\n",
    "        self.iteration = 0\n",
    "    def _interpolate(self, iter1, iter2, rate1, rate2):\n",
    "        return ((rate2 - rate1) * (self.iteration - iter1)\n",
    "                / (iter2 - iter1) + rate1)\n",
    "    def on_batch_begin(self, batch, logs):\n",
    "        if self.iteration < self.half_iteration:\n",
    "            rate = self._interpolate(0, self.half_iteration, self.start_rate, self.max_rate)\n",
    "        elif self.iteration < 2 * self.half_iteration:\n",
    "            rate = self._interpolate(self.half_iteration, 2 * self.half_iteration,\n",
    "                                     self.max_rate, self.start_rate)\n",
    "        else:\n",
    "            rate = self._interpolate(2 * self.half_iteration, self.iterations,\n",
    "                                     self.start_rate, self.last_rate)\n",
    "        self.iteration += 1\n",
    "        K.set_value(self.model.optimizer.learning_rate, rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "id": "ee3558aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "model, callbacks = cnn_mp_r3_de_v2()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "id": "aaf68050",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/2\n",
      "91/91 [==============================] - 96s 1s/step - loss: 3085.5239 - sparse_categorical_accuracy: 0.2294\n",
      "Epoch 2/2\n",
      "91/91 [==============================] - 95s 1s/step - loss: 95.5725 - sparse_categorical_accuracy: 0.2215\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "batch_size = 32\n",
    "rates, losses = find_learnig_rate(model, train_set, epochs=2, batch_size=batch_size)\n",
    "plot_rates_vs_losses(rates, losses)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8237b82a",
   "metadata": {},
   "source": [
    "##### Observations\n",
    "\n",
    "From the above graph we can set the max learning rate to arroud 0.8 for 1cycle scheduling aproch"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49d69259",
   "metadata": {},
   "source": [
    "#### Applying the one cycle aproch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "id": "8ac20759",
   "metadata": {},
   "outputs": [],
   "source": [
    "model, callbacks = cnn_mp_r3_de_v2()\n",
    "\n",
    "steps_factor = 0.85\n",
    "epochs = 5\n",
    "max_learning_rate = 0.8\n",
    "one_cycle_cb = OneCycleScheduler(int(steps_factor * max_train_instance / batch_size) * epochs, max_rate=max_learning_rate)\n",
    "\n",
    "callbacks.append(one_cycle_cb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "id": "8bef8e5d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "91/91 [==============================] - 228s 3s/step - loss: 11959.7666 - sparse_categorical_accuracy: 0.2119 - val_loss: 1.6336 - val_sparse_categorical_accuracy: 0.2392\n",
      "Epoch 2/5\n",
      "91/91 [==============================] - 226s 3s/step - loss: 1.6242 - sparse_categorical_accuracy: 0.2157 - val_loss: 1.6138 - val_sparse_categorical_accuracy: 0.1791\n",
      "Epoch 3/5\n",
      "91/91 [==============================] - 222s 2s/step - loss: 1.6184 - sparse_categorical_accuracy: 0.2648 - val_loss: 1.6402 - val_sparse_categorical_accuracy: 0.2512\n",
      "Epoch 4/5\n",
      "91/91 [==============================] - 224s 2s/step - loss: 1.6072 - sparse_categorical_accuracy: 0.2438 - val_loss: 1.6235 - val_sparse_categorical_accuracy: 0.2512\n",
      "Epoch 5/5\n",
      "91/91 [==============================] - 224s 2s/step - loss: 1.6118 - sparse_categorical_accuracy: 0.2047 - val_loss: 1.6033 - val_sparse_categorical_accuracy: 0.2512\n"
     ]
    }
   ],
   "source": [
    "history = model.fit(train_set, \n",
    "                    epochs=epochs, \n",
    "                    steps_per_epoch=int(steps_factor * max_train_instance / batch_size),\n",
    "                    validation_data=valid_set, \n",
    "                    callbacks=callbacks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "id": "a3eb4973",
   "metadata": {},
   "outputs": [],
   "source": [
    "model, callbacks = cnn_mp_r3_de_v2()\n",
    "\n",
    "steps_factor = 0.85\n",
    "epochs = 5\n",
    "max_learning_rate = 1.2\n",
    "one_cycle_cb = OneCycleScheduler(int(steps_factor * max_train_instance / batch_size) * epochs, max_rate=max_learning_rate)\n",
    "\n",
    "callbacks.append(one_cycle_cb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "id": "0de3439f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "91/91 [==============================] - 227s 2s/step - loss: 12666506.0000 - sparse_categorical_accuracy: 0.2376 - val_loss: 197.1367 - val_sparse_categorical_accuracy: 0.2509\n",
      "Epoch 2/5\n",
      "91/91 [==============================] - 225s 2s/step - loss: 33.2915 - sparse_categorical_accuracy: 0.2242 - val_loss: 1.6751 - val_sparse_categorical_accuracy: 0.2512\n",
      "Epoch 3/5\n",
      "91/91 [==============================] - 225s 2s/step - loss: 1.6692 - sparse_categorical_accuracy: 0.2294 - val_loss: 1.6285 - val_sparse_categorical_accuracy: 0.2395\n",
      "Epoch 4/5\n",
      "91/91 [==============================] - 223s 2s/step - loss: 1.6642 - sparse_categorical_accuracy: 0.2291 - val_loss: 1.6160 - val_sparse_categorical_accuracy: 0.1791\n",
      "Epoch 5/5\n",
      "91/91 [==============================] - 224s 2s/step - loss: 1.5953 - sparse_categorical_accuracy: 0.2473 - val_loss: 1.5978 - val_sparse_categorical_accuracy: 0.2512\n"
     ]
    }
   ],
   "source": [
    "history = model.fit(train_set, \n",
    "                    epochs=epochs, \n",
    "                    steps_per_epoch=int(steps_factor * max_train_instance / batch_size),\n",
    "                    validation_data=valid_set, \n",
    "                    callbacks=callbacks)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61d378c0",
   "metadata": {},
   "source": [
    "### Loading tuned model from flowers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "id": "298fb00c",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_target = os.path.join(BASE_DIR, \"models\", \"cnn_mp_r3_de_v23.h5\")\n",
    "load_model = keras.models.load_model(model_target)\n",
    "model = keras.models.clone_model(load_model)\n",
    "model.set_weights(load_model.get_weights())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "id": "3054bcaa",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_target = os.path.join(BASE_DIR, \"models\", \"cnn_mp_r3_de_v23.h5\")\n",
    "checkpoint_cb = keras.callbacks.ModelCheckpoint(model_target, save_best_only=True)\n",
    "early_stop_cb = keras.callbacks.EarlyStopping(patience=10)\n",
    "callbacks = [checkpoint_cb, early_stop_cb]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "id": "89591f23",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compile_model(model):\n",
    "    optimizer = keras.optimizers.Nadam(learning_rate=0.0009)\n",
    "    model.compile(loss=keras.losses.sparse_categorical_crossentropy, \n",
    "                  optimizer=optimizer,\n",
    "                  metrics=keras.metrics.sparse_categorical_accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "id": "173e2bb1",
   "metadata": {},
   "outputs": [],
   "source": [
    "compile_model(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "id": "a45c30e4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n",
      "91/91 [==============================] - 291s 3s/step - loss: 1.3286 - sparse_categorical_accuracy: 0.4200 - val_loss: 1.1607 - val_sparse_categorical_accuracy: 0.4820\n",
      "Epoch 2/50\n",
      "91/91 [==============================] - 224s 2s/step - loss: 1.1828 - sparse_categorical_accuracy: 0.5282 - val_loss: 1.1286 - val_sparse_categorical_accuracy: 0.5459\n",
      "Epoch 3/50\n",
      "91/91 [==============================] - 220s 2s/step - loss: 1.1200 - sparse_categorical_accuracy: 0.5649 - val_loss: 1.0689 - val_sparse_categorical_accuracy: 0.5698\n",
      "Epoch 4/50\n",
      "91/91 [==============================] - 219s 2s/step - loss: 1.0705 - sparse_categorical_accuracy: 0.5735 - val_loss: 1.0686 - val_sparse_categorical_accuracy: 0.5884\n",
      "Epoch 5/50\n",
      "91/91 [==============================] - 218s 2s/step - loss: 1.0348 - sparse_categorical_accuracy: 0.5958 - val_loss: 0.9779 - val_sparse_categorical_accuracy: 0.6285\n",
      "Epoch 6/50\n",
      "91/91 [==============================] - 219s 2s/step - loss: 1.0146 - sparse_categorical_accuracy: 0.6051 - val_loss: 1.0058 - val_sparse_categorical_accuracy: 0.6119\n",
      "Epoch 7/50\n",
      "91/91 [==============================] - 218s 2s/step - loss: 1.0263 - sparse_categorical_accuracy: 0.6058 - val_loss: 0.9470 - val_sparse_categorical_accuracy: 0.6201\n",
      "Epoch 8/50\n",
      "91/91 [==============================] - 218s 2s/step - loss: 0.9639 - sparse_categorical_accuracy: 0.6154 - val_loss: 0.9873 - val_sparse_categorical_accuracy: 0.6180\n",
      "Epoch 9/50\n",
      "91/91 [==============================] - 219s 2s/step - loss: 0.9721 - sparse_categorical_accuracy: 0.6229 - val_loss: 0.9505 - val_sparse_categorical_accuracy: 0.6160\n",
      "Epoch 10/50\n",
      "36/91 [==========>...................] - ETA: 52s - loss: 0.9241 - sparse_categorical_accuracy: 0.6406WARNING:tensorflow:Your input ran out of data; interrupting training. Make sure that your dataset or generator can generate at least `steps_per_epoch * epochs` batches (in this case, 4550 batches). You may need to use the repeat() function when building your dataset.\n",
      "91/91 [==============================] - 51248s 569s/step - loss: 0.9241 - sparse_categorical_accuracy: 0.6406 - val_loss: 0.9306 - val_sparse_categorical_accuracy: 0.6372\n"
     ]
    }
   ],
   "source": [
    "epochs = 50\n",
    "history = model.fit(train_set, \n",
    "                    epochs=epochs, \n",
    "                    steps_per_epoch=int(steps_factor * max_train_instance / batch_size),\n",
    "                    validation_data=valid_set, \n",
    "                    callbacks=callbacks,\n",
    "                    initial_epoch=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5282563",
   "metadata": {},
   "source": [
    "##### Note\n",
    "\n",
    "Added repeat to validation set to supress the above warning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "id": "e839d081",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_target = os.path.join(BASE_DIR, \"models\", \"cnn_mp_r3_de_v23.h5\")\n",
    "load_model = keras.models.load_model(model_target)\n",
    "model = keras.models.clone_model(load_model)\n",
    "model.set_weights(load_model.get_weights())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "id": "43a59468",
   "metadata": {},
   "outputs": [],
   "source": [
    "compile_model(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "id": "b9a0ee4f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 11/50\n",
      "91/91 [==============================] - 171s 2s/step - loss: 9.7253 - sparse_categorical_accuracy: 0.3630 - val_loss: 1.5808 - val_sparse_categorical_accuracy: 0.3726\n",
      "Epoch 12/50\n",
      "91/91 [==============================] - 163s 2s/step - loss: 1.5758 - sparse_categorical_accuracy: 0.3520 - val_loss: 1.4522 - val_sparse_categorical_accuracy: 0.3702\n",
      "Epoch 13/50\n",
      "91/91 [==============================] - 185s 2s/step - loss: 1.5023 - sparse_categorical_accuracy: 0.3462 - val_loss: 1.4363 - val_sparse_categorical_accuracy: 0.3678\n",
      "Epoch 14/50\n",
      "91/91 [==============================] - 181s 2s/step - loss: 1.4423 - sparse_categorical_accuracy: 0.3750 - val_loss: 1.4256 - val_sparse_categorical_accuracy: 0.3606\n",
      "Epoch 15/50\n",
      "91/91 [==============================] - 176s 2s/step - loss: 1.4453 - sparse_categorical_accuracy: 0.3719 - val_loss: 1.3717 - val_sparse_categorical_accuracy: 0.4255\n",
      "Epoch 16/50\n",
      "91/91 [==============================] - 169s 2s/step - loss: 1.4443 - sparse_categorical_accuracy: 0.3777 - val_loss: 1.3617 - val_sparse_categorical_accuracy: 0.4567\n",
      "Epoch 17/50\n",
      "91/91 [==============================] - 170s 2s/step - loss: 1.3892 - sparse_categorical_accuracy: 0.4148 - val_loss: 1.3258 - val_sparse_categorical_accuracy: 0.4255\n",
      "Epoch 18/50\n",
      "91/91 [==============================] - 165s 2s/step - loss: 1.4123 - sparse_categorical_accuracy: 0.4076 - val_loss: 1.3302 - val_sparse_categorical_accuracy: 0.4351\n",
      "Epoch 19/50\n",
      "91/91 [==============================] - 168s 2s/step - loss: 1.3751 - sparse_categorical_accuracy: 0.3959 - val_loss: 1.2949 - val_sparse_categorical_accuracy: 0.4447\n",
      "Epoch 20/50\n",
      "91/91 [==============================] - 171s 2s/step - loss: 1.3545 - sparse_categorical_accuracy: 0.4368 - val_loss: 1.3257 - val_sparse_categorical_accuracy: 0.4183\n",
      "Epoch 21/50\n",
      "91/91 [==============================] - 168s 2s/step - loss: 1.3636 - sparse_categorical_accuracy: 0.3911 - val_loss: 1.3726 - val_sparse_categorical_accuracy: 0.3822\n",
      "Epoch 22/50\n",
      "91/91 [==============================] - 171s 2s/step - loss: 1.3373 - sparse_categorical_accuracy: 0.4348 - val_loss: 1.3211 - val_sparse_categorical_accuracy: 0.4303\n",
      "Epoch 23/50\n",
      "91/91 [==============================] - 170s 2s/step - loss: 1.3213 - sparse_categorical_accuracy: 0.4317 - val_loss: 1.2633 - val_sparse_categorical_accuracy: 0.4591\n",
      "Epoch 24/50\n",
      "91/91 [==============================] - 174s 2s/step - loss: 1.2814 - sparse_categorical_accuracy: 0.4454 - val_loss: 1.3237 - val_sparse_categorical_accuracy: 0.4856\n",
      "Epoch 25/50\n",
      "91/91 [==============================] - 170s 2s/step - loss: 1.2952 - sparse_categorical_accuracy: 0.4475 - val_loss: 1.2862 - val_sparse_categorical_accuracy: 0.4519\n",
      "Epoch 26/50\n",
      "91/91 [==============================] - 181s 2s/step - loss: 1.2977 - sparse_categorical_accuracy: 0.4433 - val_loss: 1.2341 - val_sparse_categorical_accuracy: 0.4447\n",
      "Epoch 27/50\n",
      "91/91 [==============================] - 162s 2s/step - loss: 1.2970 - sparse_categorical_accuracy: 0.4327 - val_loss: 1.2557 - val_sparse_categorical_accuracy: 0.4447\n",
      "Epoch 28/50\n",
      "91/91 [==============================] - 164s 2s/step - loss: 1.2672 - sparse_categorical_accuracy: 0.4530 - val_loss: 1.2024 - val_sparse_categorical_accuracy: 0.4712\n",
      "Epoch 29/50\n",
      "91/91 [==============================] - 159s 2s/step - loss: 1.2848 - sparse_categorical_accuracy: 0.4612 - val_loss: 1.2268 - val_sparse_categorical_accuracy: 0.4760\n",
      "Epoch 30/50\n",
      "91/91 [==============================] - 159s 2s/step - loss: 1.2828 - sparse_categorical_accuracy: 0.4478 - val_loss: 1.1949 - val_sparse_categorical_accuracy: 0.4736\n",
      "Epoch 31/50\n",
      "91/91 [==============================] - 157s 2s/step - loss: 1.3128 - sparse_categorical_accuracy: 0.4423 - val_loss: 1.1783 - val_sparse_categorical_accuracy: 0.5024\n",
      "Epoch 32/50\n",
      "91/91 [==============================] - 161s 2s/step - loss: 1.2310 - sparse_categorical_accuracy: 0.4736 - val_loss: 1.1654 - val_sparse_categorical_accuracy: 0.5048\n",
      "Epoch 33/50\n",
      "91/91 [==============================] - 158s 2s/step - loss: 1.2430 - sparse_categorical_accuracy: 0.4883 - val_loss: 1.1938 - val_sparse_categorical_accuracy: 0.4976\n",
      "Epoch 34/50\n",
      "91/91 [==============================] - 158s 2s/step - loss: 1.2473 - sparse_categorical_accuracy: 0.4681 - val_loss: 1.1924 - val_sparse_categorical_accuracy: 0.4808\n",
      "Epoch 35/50\n",
      "91/91 [==============================] - 158s 2s/step - loss: 1.2562 - sparse_categorical_accuracy: 0.4481 - val_loss: 1.1786 - val_sparse_categorical_accuracy: 0.4952\n",
      "Epoch 36/50\n",
      "91/91 [==============================] - 158s 2s/step - loss: 1.2005 - sparse_categorical_accuracy: 0.5069 - val_loss: 1.1289 - val_sparse_categorical_accuracy: 0.5192\n",
      "Epoch 37/50\n",
      "91/91 [==============================] - 158s 2s/step - loss: 1.2145 - sparse_categorical_accuracy: 0.4876 - val_loss: 1.1263 - val_sparse_categorical_accuracy: 0.5048\n",
      "Epoch 38/50\n",
      "91/91 [==============================] - 159s 2s/step - loss: 1.1913 - sparse_categorical_accuracy: 0.4924 - val_loss: 1.2798 - val_sparse_categorical_accuracy: 0.4375\n",
      "Epoch 39/50\n",
      "91/91 [==============================] - 159s 2s/step - loss: 1.2399 - sparse_categorical_accuracy: 0.4729 - val_loss: 1.1572 - val_sparse_categorical_accuracy: 0.5216\n",
      "Epoch 40/50\n",
      "91/91 [==============================] - 158s 2s/step - loss: 1.1855 - sparse_categorical_accuracy: 0.5027 - val_loss: 1.1446 - val_sparse_categorical_accuracy: 0.5072\n",
      "Epoch 41/50\n",
      "91/91 [==============================] - 158s 2s/step - loss: 1.3395 - sparse_categorical_accuracy: 0.4770 - val_loss: 1.2580 - val_sparse_categorical_accuracy: 0.4279\n",
      "Epoch 42/50\n",
      "91/91 [==============================] - 156s 2s/step - loss: 2.3662 - sparse_categorical_accuracy: 0.2157 - val_loss: 1.6220 - val_sparse_categorical_accuracy: 0.2476\n",
      "Epoch 43/50\n",
      "91/91 [==============================] - 154s 2s/step - loss: 1.6258 - sparse_categorical_accuracy: 0.2569 - val_loss: 1.6223 - val_sparse_categorical_accuracy: 0.2524\n",
      "Epoch 44/50\n",
      "91/91 [==============================] - 155s 2s/step - loss: 1.6152 - sparse_categorical_accuracy: 0.2438 - val_loss: 1.6076 - val_sparse_categorical_accuracy: 0.2500\n",
      "Epoch 45/50\n",
      "91/91 [==============================] - 155s 2s/step - loss: 1.6143 - sparse_categorical_accuracy: 0.2503 - val_loss: 1.6019 - val_sparse_categorical_accuracy: 0.2500\n",
      "Epoch 46/50\n",
      "91/91 [==============================] - 154s 2s/step - loss: 1.6194 - sparse_categorical_accuracy: 0.2146 - val_loss: 1.6014 - val_sparse_categorical_accuracy: 0.2452\n",
      "Epoch 47/50\n",
      "91/91 [==============================] - 154s 2s/step - loss: 1.5886 - sparse_categorical_accuracy: 0.2665 - val_loss: 1.5974 - val_sparse_categorical_accuracy: 0.2500\n"
     ]
    }
   ],
   "source": [
    "epochs = 50\n",
    "history = model.fit(train_set, \n",
    "                    epochs=epochs, \n",
    "                    steps_per_epoch=int(steps_factor * max_train_instance / batch_size),\n",
    "                    validation_data=valid_set,\n",
    "                    validation_steps=int(max_valid_instance / batch_size),\n",
    "                    callbacks=callbacks,\n",
    "                    initial_epoch=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "id": "71544bd3",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_target = os.path.join(BASE_DIR, \"models\", \"cnn_mp_r3_de_v23.h5\")\n",
    "load_model = keras.models.load_model(model_target)\n",
    "model = keras.models.clone_model(load_model)\n",
    "model.set_weights(load_model.get_weights())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "id": "a370e0b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_target = os.path.join(BASE_DIR, \"models\", \"cnn_mp_r3_de_v23_lr005.h5\")\n",
    "checkpoint_cb = keras.callbacks.ModelCheckpoint(model_target, save_best_only=True)\n",
    "early_stop_cb = keras.callbacks.EarlyStopping(patience=10)\n",
    "callbacks = [checkpoint_cb, early_stop_cb]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "id": "de7cfb4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compile_model(model):\n",
    "    optimizer = keras.optimizers.Nadam(learning_rate=0.005)\n",
    "    model.compile(loss=keras.losses.sparse_categorical_crossentropy, \n",
    "                  optimizer=optimizer,\n",
    "                  metrics=keras.metrics.sparse_categorical_accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "id": "d3a0dea9",
   "metadata": {},
   "outputs": [],
   "source": [
    "compile_model(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "id": "bf0d5eb9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 11/50\n",
      "91/91 [==============================] - 136s 1s/step - loss: 12.2356 - sparse_categorical_accuracy: 0.3125 - val_loss: 1.4845 - val_sparse_categorical_accuracy: 0.3438\n",
      "Epoch 12/50\n",
      "91/91 [==============================] - 131s 1s/step - loss: 10.5128 - sparse_categorical_accuracy: 0.2685 - val_loss: 1.6000 - val_sparse_categorical_accuracy: 0.2596\n",
      "Epoch 13/50\n",
      "91/91 [==============================] - 131s 1s/step - loss: 1.6085 - sparse_categorical_accuracy: 0.2373 - val_loss: 1.5957 - val_sparse_categorical_accuracy: 0.2500\n",
      "Epoch 14/50\n",
      "91/91 [==============================] - 131s 1s/step - loss: 1.6034 - sparse_categorical_accuracy: 0.2435 - val_loss: 1.5936 - val_sparse_categorical_accuracy: 0.2500\n",
      "Epoch 15/50\n",
      "91/91 [==============================] - 131s 1s/step - loss: 3.0765 - sparse_categorical_accuracy: 0.2304 - val_loss: 1.5929 - val_sparse_categorical_accuracy: 0.2524\n",
      "Epoch 16/50\n",
      "91/91 [==============================] - 130s 1s/step - loss: 1.6059 - sparse_categorical_accuracy: 0.2294 - val_loss: 1.5965 - val_sparse_categorical_accuracy: 0.2476\n",
      "Epoch 17/50\n",
      "91/91 [==============================] - 130s 1s/step - loss: 1.7950 - sparse_categorical_accuracy: 0.2448 - val_loss: 1.5937 - val_sparse_categorical_accuracy: 0.2428\n",
      "Epoch 18/50\n",
      "91/91 [==============================] - 130s 1s/step - loss: 1.5989 - sparse_categorical_accuracy: 0.2510 - val_loss: 1.5936 - val_sparse_categorical_accuracy: 0.2524\n",
      "Epoch 19/50\n",
      "91/91 [==============================] - 139s 2s/step - loss: 1.6451 - sparse_categorical_accuracy: 0.2263 - val_loss: 1.5948 - val_sparse_categorical_accuracy: 0.2332\n",
      "Epoch 20/50\n",
      "91/91 [==============================] - 129s 1s/step - loss: 1.6029 - sparse_categorical_accuracy: 0.2579 - val_loss: 1.5911 - val_sparse_categorical_accuracy: 0.2500\n",
      "Epoch 21/50\n",
      "91/91 [==============================] - 129s 1s/step - loss: 1.6129 - sparse_categorical_accuracy: 0.2040 - val_loss: 1.5967 - val_sparse_categorical_accuracy: 0.2596\n"
     ]
    }
   ],
   "source": [
    "epochs = 50\n",
    "history = model.fit(train_set, \n",
    "                    epochs=epochs, \n",
    "                    steps_per_epoch=int(steps_factor * max_train_instance / batch_size),\n",
    "                    validation_data=valid_set,\n",
    "                    validation_steps=int(max_valid_instance / batch_size),\n",
    "                    callbacks=callbacks,\n",
    "                    initial_epoch=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c14741c",
   "metadata": {},
   "source": [
    "##### Note\n",
    "\n",
    "Benchmark is taken from flowers to make jump to SGD as optimizer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "id": "7fce1565",
   "metadata": {},
   "outputs": [],
   "source": [
    "class OneCycleSchedulerSGD(keras.callbacks.Callback):\n",
    "    def __init__(self, iterations, \n",
    "                 max_lrate, \n",
    "                 start_lrate=None,\n",
    "                 last_iterations=None, \n",
    "                 last_lrate=None,\n",
    "                 max_momentum=0.95,\n",
    "                 min_momentum=0.85):\n",
    "        \n",
    "        self.iterations = iterations\n",
    "        self.last_iterations = last_iterations or iterations // 10 + 1\n",
    "        self.half_iteration = (iterations - self.last_iterations) // 2\n",
    "        \n",
    "        self.max_lrate = max_lrate\n",
    "        self.start_lrate = start_lrate or max_lrate / 10\n",
    "        self.last_lrate = last_lrate or self.start_lrate / 1000\n",
    "        \n",
    "        self.max_momentum = max_momentum\n",
    "        self.min_momentum = min_momentum\n",
    "        self.last_momentum = max_momentum\n",
    "        \n",
    "        self.iteration = 0\n",
    "        \n",
    "        self.rate = []\n",
    "        self.momentum = []\n",
    "\n",
    "        self.loss = []\n",
    "        self.val_loss = []\n",
    "        self.accuracy = []\n",
    "        self.val_accuracy = []\n",
    "    \n",
    "    def _interpolate(self, iter1, iter2, lrate1, lrate2):\n",
    "        return ((lrate2 - lrate1) * (self.iteration - iter1)\n",
    "                / (iter2 - iter1) + lrate1)\n",
    "    \n",
    "    def on_batch_begin(self, batch, logs):\n",
    "        if self.iteration < self.half_iteration:\n",
    "            rate = self._interpolate(0, self.half_iteration, self.start_lrate, self.max_lrate)\n",
    "            momentum = self._interpolate(0, self.half_iteration, self.max_momentum, self.min_momentum)\n",
    "        elif self.iteration < 2 * self.half_iteration:\n",
    "            rate = self._interpolate(self.half_iteration, 2 * self.half_iteration, self.max_lrate, self.start_lrate)\n",
    "            momentum = self._interpolate(self.half_iteration, 2 * self.half_iteration, self.min_momentum, self.max_momentum)\n",
    "        else:\n",
    "            rate = self._interpolate(2 * self.half_iteration, self.iterations, self.start_lrate, self.last_lrate)\n",
    "            momentum = self.last_momentum\n",
    "            \n",
    "        self.iteration += 1\n",
    "        K.set_value(self.model.optimizer.learning_rate, rate)\n",
    "        K.set_value(self.model.optimizer.momentum, momentum)\n",
    "        \n",
    "    def on_batch_end(self, batch, logs):\n",
    "        self.rate.append(K.get_value(self.model.optimizer.learning_rate))\n",
    "        self.momentum.append(K.get_value(self.model.optimizer.momentum))\n",
    "\n",
    "        self.loss.append(logs[\"loss\"])\n",
    "        self.accuracy.append(logs[\"sparse_categorical_accuracy\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "id": "0f6f1282",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_target = os.path.join(BASE_DIR, \"models\", \"cnn_mp_r3_de_v22.h5\")\n",
    "load_model = keras.models.load_model(model_target)\n",
    "model = keras.models.clone_model(load_model)\n",
    "model.set_weights(load_model.get_weights())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "id": "6272447d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compile_model(model):\n",
    "    optimizer = keras.optimizers.SGD(nesterov=True)\n",
    "    model.compile(loss=keras.losses.sparse_categorical_crossentropy, \n",
    "                  optimizer=optimizer,\n",
    "                  metrics=keras.metrics.sparse_categorical_accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "id": "e6454e5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "compile_model(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "id": "3d77d5e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_target = os.path.join(BASE_DIR, \"models\", \"cnn_mp_r3_de_v22_1c_sgd_nes.h5\")\n",
    "\n",
    "checkpoint_cb = keras.callbacks.ModelCheckpoint(model_target, save_best_only=True)\n",
    "early_stop_cb = keras.callbacks.EarlyStopping(patience=10)\n",
    "\n",
    "callbacks = [checkpoint_cb, early_stop_cb]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "id": "9d5132e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs = 50\n",
    "max_learning_rate = 0.01\n",
    "onecycle_cb = OneCycleSchedulerSGD (int(steps_factor * max_train_instance / batch_size) * epochs, max_lrate=max_learning_rate)\n",
    "\n",
    "callbacks.append(onecycle_cb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "id": "2e18a29f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n",
      "91/91 [==============================] - 158s 2s/step - loss: 1.0810 - sparse_categorical_accuracy: 0.5807 - val_loss: 1.3357 - val_sparse_categorical_accuracy: 0.5385\n",
      "Epoch 2/50\n",
      "91/91 [==============================] - 155s 2s/step - loss: 1.0622 - sparse_categorical_accuracy: 0.5725 - val_loss: 0.9774 - val_sparse_categorical_accuracy: 0.6202\n",
      "Epoch 3/50\n",
      "91/91 [==============================] - 154s 2s/step - loss: 1.0935 - sparse_categorical_accuracy: 0.5450 - val_loss: 1.0018 - val_sparse_categorical_accuracy: 0.5841\n",
      "Epoch 4/50\n",
      "91/91 [==============================] - 144s 2s/step - loss: 1.0556 - sparse_categorical_accuracy: 0.5731 - val_loss: 0.9838 - val_sparse_categorical_accuracy: 0.5625\n",
      "Epoch 5/50\n",
      "91/91 [==============================] - 124s 1s/step - loss: 1.0855 - sparse_categorical_accuracy: 0.5659 - val_loss: 0.9641 - val_sparse_categorical_accuracy: 0.6274\n",
      "Epoch 6/50\n",
      "91/91 [==============================] - 124s 1s/step - loss: 1.1059 - sparse_categorical_accuracy: 0.5536 - val_loss: 1.0196 - val_sparse_categorical_accuracy: 0.5793\n",
      "Epoch 7/50\n",
      "91/91 [==============================] - 125s 1s/step - loss: 1.0733 - sparse_categorical_accuracy: 0.5766 - val_loss: 1.0638 - val_sparse_categorical_accuracy: 0.5817\n",
      "Epoch 8/50\n",
      "91/91 [==============================] - 124s 1s/step - loss: 1.0653 - sparse_categorical_accuracy: 0.5824 - val_loss: 0.9748 - val_sparse_categorical_accuracy: 0.5986\n",
      "Epoch 9/50\n",
      "91/91 [==============================] - 124s 1s/step - loss: 1.0946 - sparse_categorical_accuracy: 0.5625 - val_loss: 0.9702 - val_sparse_categorical_accuracy: 0.6298\n",
      "Epoch 10/50\n",
      "91/91 [==============================] - 123s 1s/step - loss: 1.0905 - sparse_categorical_accuracy: 0.5536 - val_loss: 1.0209 - val_sparse_categorical_accuracy: 0.6082\n",
      "Epoch 11/50\n",
      "91/91 [==============================] - 125s 1s/step - loss: 1.1249 - sparse_categorical_accuracy: 0.5361 - val_loss: 1.0702 - val_sparse_categorical_accuracy: 0.5697\n",
      "Epoch 12/50\n",
      "91/91 [==============================] - 124s 1s/step - loss: 1.0956 - sparse_categorical_accuracy: 0.5608 - val_loss: 1.0108 - val_sparse_categorical_accuracy: 0.5793\n",
      "Epoch 13/50\n",
      "91/91 [==============================] - 124s 1s/step - loss: 1.1139 - sparse_categorical_accuracy: 0.5422 - val_loss: 1.0152 - val_sparse_categorical_accuracy: 0.6130\n",
      "Epoch 14/50\n",
      "91/91 [==============================] - 124s 1s/step - loss: 1.1325 - sparse_categorical_accuracy: 0.5333 - val_loss: 1.0014 - val_sparse_categorical_accuracy: 0.6058\n",
      "Epoch 15/50\n",
      "91/91 [==============================] - 124s 1s/step - loss: 1.1055 - sparse_categorical_accuracy: 0.5591 - val_loss: 1.0328 - val_sparse_categorical_accuracy: 0.5817\n"
     ]
    }
   ],
   "source": [
    "history = model.fit(train_set, \n",
    "                    epochs=epochs, \n",
    "                    steps_per_epoch=int(steps_factor * max_train_instance / batch_size),\n",
    "                    validation_data=valid_set,\n",
    "                    validation_steps=int(max_valid_instance / batch_size),\n",
    "                    callbacks=callbacks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "id": "e899db4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "summary = {\"lrate\": onecycle_cb.rate,\n",
    "           \"b1\": onecycle_cb.momentum,\n",
    "           \"loss\": onecycle_cb.loss,\n",
    "           \"accuracy\": onecycle_cb.accuracy}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "id": "f2434d11",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 576x864 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "fig, axs = plt.subplots(2, figsize=(8, 12))\n",
    "axs[0].plot(summary[\"lrate\"], summary[\"loss\"])\n",
    "axs[0].legend([\"lrate Vs loss\"])\n",
    "axs[1].plot(summary[\"lrate\"], summary[\"accuracy\"])\n",
    "axs[1].legend([\"lrate Vs accuracy\"])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 182,
   "id": "b82853e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "class OneCycleSchedulerNadam(keras.callbacks.Callback):\n",
    "    def __init__(self, iterations, \n",
    "                 max_lrate, \n",
    "                 start_lrate=None,\n",
    "                 last_iterations=None, \n",
    "                 last_lrate=None,\n",
    "                 max_b1rate=0.95,\n",
    "                 min_b1rate=0.85,\n",
    "                 max_b2rate=0.9995,\n",
    "                 min_b2rate=0.9985):\n",
    "        \n",
    "        self.iterations = iterations\n",
    "        self.last_iterations = last_iterations or iterations // 10 + 1\n",
    "        self.half_iteration = (iterations - self.last_iterations) // 2\n",
    "        \n",
    "        self.max_lrate = max_lrate\n",
    "        self.start_lrate = start_lrate or max_lrate / 10\n",
    "        self.last_lrate = last_lrate or self.start_lrate / 1000\n",
    "        \n",
    "        self.max_b1rate = max_b1rate\n",
    "        self.min_b1rate = min_b1rate\n",
    "        self.last_b1rate = max_b1rate\n",
    "        \n",
    "        self.max_b2rate = max_b2rate\n",
    "        self.min_b2rate = min_b2rate\n",
    "        self.last_b2rate = max_b2rate\n",
    "\n",
    "        self.iteration = 0\n",
    "        \n",
    "        self.rate = []\n",
    "        self.b1 = []\n",
    "        self.b2 = []\n",
    "\n",
    "        self.loss = []\n",
    "        self.val_loss = []\n",
    "        self.accuracy = []\n",
    "        self.val_accuracy = []\n",
    "    \n",
    "    def _interpolate(self, iter1, iter2, lrate1, lrate2):\n",
    "        return ((lrate2 - lrate1) * (self.iteration - iter1)\n",
    "                / (iter2 - iter1) + lrate1)\n",
    "    \n",
    "    def on_batch_begin(self, batch, logs):\n",
    "        if self.iteration < self.half_iteration:\n",
    "            rate = self._interpolate(0, self.half_iteration, self.start_lrate, self.max_lrate)\n",
    "            b1 = self._interpolate(0, self.half_iteration, self.max_b1rate, self.min_b1rate)\n",
    "            b2 = self._interpolate(0, self.half_iteration, self.max_b2rate, self.min_b2rate)\n",
    "        elif self.iteration < 2 * self.half_iteration:\n",
    "            rate = self._interpolate(self.half_iteration, 2 * self.half_iteration, self.max_lrate, self.start_lrate)\n",
    "            b1 = self._interpolate(self.half_iteration, 2 * self.half_iteration, self.min_b1rate, self.max_b1rate)\n",
    "            b2 = self._interpolate(self.half_iteration, 2 * self.half_iteration, self.min_b2rate, self.max_b2rate)\n",
    "        else:\n",
    "            rate = self._interpolate(2 * self.half_iteration, self.iterations, self.start_lrate, self.last_lrate)\n",
    "            b1 = self.last_b1rate\n",
    "            b2 = self.last_b2rate\n",
    "            \n",
    "        self.iteration += 1\n",
    "        K.set_value(self.model.optimizer.learning_rate, rate)\n",
    "        K.set_value(self.model.optimizer.beta_1, b1)\n",
    "        K.set_value(self.model.optimizer.beta_2, b2)\n",
    "        \n",
    "    def on_batch_end(self, batch, logs):\n",
    "        self.rate.append(K.get_value(self.model.optimizer.learning_rate))\n",
    "        self.b1.append(K.get_value(self.model.optimizer.beta_1))\n",
    "        self.b2.append(K.get_value(self.model.optimizer.beta_2))\n",
    "\n",
    "        self.loss.append(logs[\"loss\"])\n",
    "        self.accuracy.append(logs[\"sparse_categorical_accuracy\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 194,
   "id": "773e87d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "keras.backend.clear_session()\n",
    "np.random.seed(42)\n",
    "tf.random.set_seed(42)\n",
    "\n",
    "def mini_resnet_9cl():\n",
    "    \n",
    "    model = keras.models.Sequential()\n",
    "    model.add(keras.layers.Conv2D(filters=64,\n",
    "                                kernel_size=5, \n",
    "                                strides=2, \n",
    "                                padding=\"same\", \n",
    "                                use_bias=False, \n",
    "                                input_shape=[dims[0], dims[1], channels]))\n",
    "    model.add(keras.layers.BatchNormalization())\n",
    "    model.add(keras.layers.Activation(\"relu\"))\n",
    "    model.add(keras.layers.MaxPool2D(pool_size=3, strides=2, padding=\"same\"))\n",
    "    pfm = 64\n",
    "    for fm in [64, 64, 128, 128]:\n",
    "        strides = 1 if pfm == fm else 2\n",
    "        model.add(ResidualLayer(fm=fm, strides=strides))\n",
    "        pfm = fm\n",
    "    model.add(keras.layers.GlobalAvgPool2D())\n",
    "    model.add(keras.layers.Flatten())\n",
    "    model.add(keras.layers.Dense(5, activation=\"softmax\"))\n",
    "           \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 195,
   "id": "01793a8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = mini_resnet_9cl()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 196,
   "id": "ceed5f32",
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = keras.optimizers.Nadam()\n",
    "model.compile(loss=keras.losses.sparse_categorical_crossentropy, \n",
    "                  optimizer=optimizer, \n",
    "                  metrics=keras.metrics.sparse_categorical_accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 197,
   "id": "e1d7c9fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_target = os.path.join(BASE_DIR, \"models\", \"mini_resnet_9cl_nadam_1cycle.h5\")\n",
    "checkpoint_cb = keras.callbacks.ModelCheckpoint(model_target, save_best_only=True)\n",
    "early_stop_cb = keras.callbacks.EarlyStopping(patience=10)\n",
    "callbacks = [checkpoint_cb, early_stop_cb]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 198,
   "id": "87117283",
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs = 25\n",
    "max_learning_rate = 0.001\n",
    "onecycle_cb = OneCycle SchedulerNadam(int(steps_factor * max_train_instance / batch_size) * epochs, max_lrate=max_learning_rate)\n",
    "\n",
    "callbacks.append(onecycle_cb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 199,
   "id": "701668a6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/25\n",
      "91/91 [==============================] - 229s 2s/step - loss: 1.2529 - sparse_categorical_accuracy: 0.4801 - val_loss: 2.3152 - val_sparse_categorical_accuracy: 0.2524\n",
      "Epoch 2/25\n",
      "91/91 [==============================] - 219s 2s/step - loss: 1.1210 - sparse_categorical_accuracy: 0.5419 - val_loss: 2.5931 - val_sparse_categorical_accuracy: 0.2620\n",
      "Epoch 3/25\n",
      "91/91 [==============================] - 227s 3s/step - loss: 1.0868 - sparse_categorical_accuracy: 0.5622 - val_loss: 2.5422 - val_sparse_categorical_accuracy: 0.2957\n",
      "Epoch 4/25\n",
      "91/91 [==============================] - 221s 2s/step - loss: 1.0150 - sparse_categorical_accuracy: 0.6082 - val_loss: 1.2896 - val_sparse_categorical_accuracy: 0.4880\n",
      "Epoch 5/25\n",
      "91/91 [==============================] - 222s 2s/step - loss: 0.9970 - sparse_categorical_accuracy: 0.6147 - val_loss: 1.5739 - val_sparse_categorical_accuracy: 0.4375\n",
      "Epoch 6/25\n",
      "91/91 [==============================] - 218s 2s/step - loss: 0.9766 - sparse_categorical_accuracy: 0.6195 - val_loss: 1.3870 - val_sparse_categorical_accuracy: 0.4928\n",
      "Epoch 7/25\n",
      "91/91 [==============================] - 217s 2s/step - loss: 0.9652 - sparse_categorical_accuracy: 0.6250 - val_loss: 1.1167 - val_sparse_categorical_accuracy: 0.5361\n",
      "Epoch 8/25\n",
      "91/91 [==============================] - 220s 2s/step - loss: 0.9417 - sparse_categorical_accuracy: 0.6322 - val_loss: 1.3415 - val_sparse_categorical_accuracy: 0.3654\n",
      "Epoch 9/25\n",
      "91/91 [==============================] - 219s 2s/step - loss: 0.9230 - sparse_categorical_accuracy: 0.6518 - val_loss: 1.1064 - val_sparse_categorical_accuracy: 0.5745\n",
      "Epoch 10/25\n",
      "91/91 [==============================] - 217s 2s/step - loss: 0.9066 - sparse_categorical_accuracy: 0.6477 - val_loss: 1.5678 - val_sparse_categorical_accuracy: 0.3606\n",
      "Epoch 11/25\n",
      "91/91 [==============================] - 219s 2s/step - loss: 0.9088 - sparse_categorical_accuracy: 0.6429 - val_loss: 1.3993 - val_sparse_categorical_accuracy: 0.3726\n",
      "Epoch 12/25\n",
      "91/91 [==============================] - 220s 2s/step - loss: 0.8512 - sparse_categorical_accuracy: 0.6645 - val_loss: 1.5188 - val_sparse_categorical_accuracy: 0.3870\n",
      "Epoch 13/25\n",
      "91/91 [==============================] - 223s 2s/step - loss: 0.8457 - sparse_categorical_accuracy: 0.6834 - val_loss: 1.1760 - val_sparse_categorical_accuracy: 0.4856\n",
      "Epoch 14/25\n",
      "91/91 [==============================] - 223s 2s/step - loss: 0.8228 - sparse_categorical_accuracy: 0.6799 - val_loss: 1.3943 - val_sparse_categorical_accuracy: 0.4303\n",
      "Epoch 15/25\n",
      "91/91 [==============================] - 222s 2s/step - loss: 0.8083 - sparse_categorical_accuracy: 0.6947 - val_loss: 1.3222 - val_sparse_categorical_accuracy: 0.3630\n",
      "Epoch 16/25\n",
      "91/91 [==============================] - 219s 2s/step - loss: 0.7843 - sparse_categorical_accuracy: 0.6933 - val_loss: 1.0756 - val_sparse_categorical_accuracy: 0.6202\n",
      "Epoch 17/25\n",
      "91/91 [==============================] - 226s 2s/step - loss: 0.7562 - sparse_categorical_accuracy: 0.7081 - val_loss: 1.2096 - val_sparse_categorical_accuracy: 0.4712\n",
      "Epoch 18/25\n",
      "91/91 [==============================] - 222s 2s/step - loss: 0.7402 - sparse_categorical_accuracy: 0.7273 - val_loss: 1.1639 - val_sparse_categorical_accuracy: 0.5120\n",
      "Epoch 19/25\n",
      "91/91 [==============================] - 219s 2s/step - loss: 0.7100 - sparse_categorical_accuracy: 0.7325 - val_loss: 1.5891 - val_sparse_categorical_accuracy: 0.3462\n",
      "Epoch 20/25\n",
      "91/91 [==============================] - 220s 2s/step - loss: 0.6694 - sparse_categorical_accuracy: 0.7534 - val_loss: 1.2284 - val_sparse_categorical_accuracy: 0.4760\n",
      "Epoch 21/25\n",
      "91/91 [==============================] - 218s 2s/step - loss: 0.6657 - sparse_categorical_accuracy: 0.7552 - val_loss: 1.5685 - val_sparse_categorical_accuracy: 0.3413\n",
      "Epoch 22/25\n",
      "91/91 [==============================] - 222s 2s/step - loss: 0.6426 - sparse_categorical_accuracy: 0.7514 - val_loss: 1.3231 - val_sparse_categorical_accuracy: 0.4207\n",
      "Epoch 23/25\n",
      "91/91 [==============================] - 224s 2s/step - loss: 0.6223 - sparse_categorical_accuracy: 0.7661 - val_loss: 1.5777 - val_sparse_categorical_accuracy: 0.3534\n",
      "Epoch 24/25\n",
      "91/91 [==============================] - 230s 3s/step - loss: 0.6159 - sparse_categorical_accuracy: 0.7651 - val_loss: 1.5379 - val_sparse_categorical_accuracy: 0.3606\n",
      "Epoch 25/25\n",
      "91/91 [==============================] - 226s 2s/step - loss: 0.5962 - sparse_categorical_accuracy: 0.7778 - val_loss: 1.5612 - val_sparse_categorical_accuracy: 0.3582\n"
     ]
    }
   ],
   "source": [
    "history = model.fit(train_set, \n",
    "                    epochs=epochs, \n",
    "                    steps_per_epoch=int(steps_factor * max_train_instance / batch_size),\n",
    "                    validation_data=valid_set,\n",
    "                    validation_steps=int(max_valid_instance / batch_size),\n",
    "                    callbacks=callbacks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 200,
   "id": "9ea2e902",
   "metadata": {},
   "outputs": [],
   "source": [
    "keras.backend.clear_session()\n",
    "np.random.seed(42)\n",
    "tf.random.set_seed(42)\n",
    "\n",
    "def mini_resnet_7cl():\n",
    "    \n",
    "    model = keras.models.Sequential()\n",
    "    model.add(keras.layers.Conv2D(filters=64,\n",
    "                                kernel_size=5, \n",
    "                                strides=2, \n",
    "                                padding=\"same\", \n",
    "                                use_bias=False, \n",
    "                                input_shape=[dims[0], dims[1], channels]))\n",
    "    model.add(keras.layers.BatchNormalization())\n",
    "    model.add(keras.layers.Activation(\"relu\"))\n",
    "    model.add(keras.layers.MaxPool2D(pool_size=3, strides=2, padding=\"same\"))\n",
    "    pfm = 64\n",
    "    for fm in [64, 64, 64]:\n",
    "        strides = 1 if pfm == fm else 2\n",
    "        model.add(ResidualLayer(fm=fm, strides=strides))\n",
    "        pfm = fm\n",
    "    model.add(keras.layers.GlobalAvgPool2D())\n",
    "    model.add(keras.layers.Flatten())\n",
    "    model.add(keras.layers.Dense(5, activation=\"softmax\"))\n",
    "           \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 201,
   "id": "adc83ab8",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = mini_resnet_7cl()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 202,
   "id": "c885fb5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = keras.optimizers.Nadam()\n",
    "model.compile(loss=keras.losses.sparse_categorical_crossentropy, \n",
    "                  optimizer=optimizer, \n",
    "                  metrics=keras.metrics.sparse_categorical_accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 203,
   "id": "e0acf161",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_target = os.path.join(BASE_DIR, \"models\", \"mini_resnet_7cl_nadam_1cycle.h5\")\n",
    "checkpoint_cb = keras.callbacks.ModelCheckpoint(model_target, save_best_only=True)\n",
    "early_stop_cb = keras.callbacks.EarlyStopping(patience=10)\n",
    "callbacks = [checkpoint_cb, early_stop_cb]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 204,
   "id": "735872f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs = 25\n",
    "max_learning_rate = 0.001\n",
    "onecycle_cb = OneCycleSchedulerNadam(int(steps_factor * max_train_instance / batch_size) * epochs, max_lrate=max_learning_rate)\n",
    "\n",
    "callbacks.append(onecycle_cb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 205,
   "id": "9ffbf75d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/25\n",
      "91/91 [==============================] - 215s 2s/step - loss: 1.6043 - sparse_categorical_accuracy: 0.4227 - val_loss: 3.2268 - val_sparse_categorical_accuracy: 0.3510\n",
      "Epoch 2/25\n",
      "91/91 [==============================] - 210s 2s/step - loss: 1.1646 - sparse_categorical_accuracy: 0.5268 - val_loss: 2.1866 - val_sparse_categorical_accuracy: 0.4159\n",
      "Epoch 3/25\n",
      "91/91 [==============================] - 222s 2s/step - loss: 1.1334 - sparse_categorical_accuracy: 0.5512 - val_loss: 1.8089 - val_sparse_categorical_accuracy: 0.3822\n",
      "Epoch 4/25\n",
      "91/91 [==============================] - 208s 2s/step - loss: 1.0949 - sparse_categorical_accuracy: 0.5598 - val_loss: 1.7691 - val_sparse_categorical_accuracy: 0.4062\n",
      "Epoch 5/25\n",
      "91/91 [==============================] - 210s 2s/step - loss: 1.0875 - sparse_categorical_accuracy: 0.5807 - val_loss: 1.2856 - val_sparse_categorical_accuracy: 0.4399\n",
      "Epoch 6/25\n",
      "91/91 [==============================] - 209s 2s/step - loss: 1.0395 - sparse_categorical_accuracy: 0.5855 - val_loss: 1.4078 - val_sparse_categorical_accuracy: 0.4495\n",
      "Epoch 7/25\n",
      "91/91 [==============================] - 209s 2s/step - loss: 1.0556 - sparse_categorical_accuracy: 0.5852 - val_loss: 1.3574 - val_sparse_categorical_accuracy: 0.4471\n",
      "Epoch 8/25\n",
      "91/91 [==============================] - 174s 2s/step - loss: 0.9951 - sparse_categorical_accuracy: 0.6123 - val_loss: 1.4289 - val_sparse_categorical_accuracy: 0.4615\n",
      "Epoch 9/25\n",
      "91/91 [==============================] - 172s 2s/step - loss: 1.0051 - sparse_categorical_accuracy: 0.6068 - val_loss: 1.2871 - val_sparse_categorical_accuracy: 0.5096\n",
      "Epoch 10/25\n",
      "91/91 [==============================] - 172s 2s/step - loss: 0.9686 - sparse_categorical_accuracy: 0.6078 - val_loss: 1.2596 - val_sparse_categorical_accuracy: 0.4303\n",
      "Epoch 11/25\n",
      "91/91 [==============================] - 172s 2s/step - loss: 0.9511 - sparse_categorical_accuracy: 0.6195 - val_loss: 1.3894 - val_sparse_categorical_accuracy: 0.4279\n",
      "Epoch 12/25\n",
      "91/91 [==============================] - 172s 2s/step - loss: 0.9511 - sparse_categorical_accuracy: 0.6243 - val_loss: 1.1472 - val_sparse_categorical_accuracy: 0.5216\n",
      "Epoch 13/25\n",
      "91/91 [==============================] - 172s 2s/step - loss: 0.9287 - sparse_categorical_accuracy: 0.6319 - val_loss: 1.2118 - val_sparse_categorical_accuracy: 0.5072\n",
      "Epoch 14/25\n",
      "91/91 [==============================] - 173s 2s/step - loss: 0.8869 - sparse_categorical_accuracy: 0.6648 - val_loss: 1.3524 - val_sparse_categorical_accuracy: 0.4447\n",
      "Epoch 15/25\n",
      "91/91 [==============================] - 172s 2s/step - loss: 0.8809 - sparse_categorical_accuracy: 0.6604 - val_loss: 1.1332 - val_sparse_categorical_accuracy: 0.5433\n",
      "Epoch 16/25\n",
      "91/91 [==============================] - 173s 2s/step - loss: 0.8419 - sparse_categorical_accuracy: 0.6834 - val_loss: 1.2676 - val_sparse_categorical_accuracy: 0.4663\n",
      "Epoch 17/25\n",
      "91/91 [==============================] - 172s 2s/step - loss: 0.8311 - sparse_categorical_accuracy: 0.6823 - val_loss: 1.4505 - val_sparse_categorical_accuracy: 0.4087\n",
      "Epoch 18/25\n",
      "91/91 [==============================] - 3156s 35s/step - loss: 0.8274 - sparse_categorical_accuracy: 0.6865 - val_loss: 1.1334 - val_sparse_categorical_accuracy: 0.5264\n",
      "Epoch 19/25\n",
      "91/91 [==============================] - 216s 2s/step - loss: 0.7930 - sparse_categorical_accuracy: 0.6999 - val_loss: 1.0909 - val_sparse_categorical_accuracy: 0.5433\n",
      "Epoch 20/25\n",
      "91/91 [==============================] - 192s 2s/step - loss: 0.7871 - sparse_categorical_accuracy: 0.6971 - val_loss: 1.2118 - val_sparse_categorical_accuracy: 0.5216\n",
      "Epoch 21/25\n",
      "91/91 [==============================] - 179s 2s/step - loss: 0.7641 - sparse_categorical_accuracy: 0.7064 - val_loss: 1.2190 - val_sparse_categorical_accuracy: 0.5072\n",
      "Epoch 22/25\n",
      "91/91 [==============================] - 176s 2s/step - loss: 0.7464 - sparse_categorical_accuracy: 0.7184 - val_loss: 1.1674 - val_sparse_categorical_accuracy: 0.5216\n",
      "Epoch 23/25\n",
      "91/91 [==============================] - 175s 2s/step - loss: 0.7457 - sparse_categorical_accuracy: 0.7139 - val_loss: 1.2380 - val_sparse_categorical_accuracy: 0.5024\n",
      "Epoch 24/25\n",
      "91/91 [==============================] - 177s 2s/step - loss: 0.7326 - sparse_categorical_accuracy: 0.7194 - val_loss: 1.1807 - val_sparse_categorical_accuracy: 0.5096\n",
      "Epoch 25/25\n",
      "91/91 [==============================] - 175s 2s/step - loss: 0.7119 - sparse_categorical_accuracy: 0.7342 - val_loss: 1.1749 - val_sparse_categorical_accuracy: 0.5096\n"
     ]
    }
   ],
   "source": [
    "history = model.fit(train_set, \n",
    "                    epochs=epochs, \n",
    "                    steps_per_epoch=int(steps_factor * max_train_instance / batch_size),\n",
    "                    validation_data=valid_set,\n",
    "                    validation_steps=int(max_valid_instance / batch_size),\n",
    "                    callbacks=callbacks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 206,
   "id": "3dec885a",
   "metadata": {},
   "outputs": [],
   "source": [
    "keras.backend.clear_session()\n",
    "np.random.seed(42)\n",
    "tf.random.set_seed(42)\n",
    "\n",
    "def mini_resnet_5cl_de():\n",
    "    \n",
    "    model = keras.models.Sequential()\n",
    "    model.add(keras.layers.Conv2D(filters=64,\n",
    "                                kernel_size=5, \n",
    "                                strides=2, \n",
    "                                padding=\"same\", \n",
    "                                use_bias=False, \n",
    "                                input_shape=[dims[0], dims[1], channels]))\n",
    "    model.add(keras.layers.BatchNormalization())\n",
    "    model.add(keras.layers.Activation(\"relu\"))\n",
    "    model.add(keras.layers.MaxPool2D(pool_size=3, strides=2, padding=\"same\"))\n",
    "    pfm = 64\n",
    "    for fm in [64, 64]:\n",
    "        strides = 1 if pfm == fm else 2\n",
    "        model.add(ResidualLayer(fm=fm, strides=strides))\n",
    "        pfm = fm\n",
    "    model.add(keras.layers.GlobalAvgPool2D())\n",
    "    model.add(keras.layers.Flatten())\n",
    "    model.add(keras.layers.Dropout(0.2))\n",
    "    model.add(keras.layers.Dense(64, activation=\"relu\"))\n",
    "    model.add(keras.layers.Dropout(0.2))\n",
    "    model.add(keras.layers.Dense(5, activation=\"softmax\"))\n",
    "           \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 207,
   "id": "e248c2d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = mini_resnet_5cl_de()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 208,
   "id": "062e5864",
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = keras.optimizers.Nadam()\n",
    "model.compile(loss=keras.losses.sparse_categorical_crossentropy, \n",
    "                  optimizer=optimizer, \n",
    "                  metrics=keras.metrics.sparse_categorical_accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 209,
   "id": "47dddf2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_target = os.path.join(BASE_DIR, \"models\", \"mini_resnet_5cl_de_nadam_1cycle.h5\")\n",
    "checkpoint_cb = keras.callbacks.ModelCheckpoint(model_target, save_best_only=True)\n",
    "early_stop_cb = keras.callbacks.EarlyStopping(patience=10)\n",
    "callbacks = [checkpoint_cb, early_stop_cb]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 210,
   "id": "1105f35c",
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs = 25\n",
    "max_learning_rate = 0.001\n",
    "onecycle_cb = OneCycleSchedulerNadam(int(steps_factor * max_train_instance / batch_size) * epochs, max_lrate=max_learning_rate)\n",
    "\n",
    "callbacks.append(onecycle_cb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 211,
   "id": "39e6d236",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/25\n",
      "91/91 [==============================] - 157s 2s/step - loss: 1.5470 - sparse_categorical_accuracy: 0.3434 - val_loss: 1.4556 - val_sparse_categorical_accuracy: 0.4038\n",
      "Epoch 2/25\n",
      "91/91 [==============================] - 153s 2s/step - loss: 1.3323 - sparse_categorical_accuracy: 0.4368 - val_loss: 1.2791 - val_sparse_categorical_accuracy: 0.4447\n",
      "Epoch 3/25\n",
      "91/91 [==============================] - 157s 2s/step - loss: 1.2762 - sparse_categorical_accuracy: 0.4519 - val_loss: 1.3265 - val_sparse_categorical_accuracy: 0.4255\n",
      "Epoch 4/25\n",
      "91/91 [==============================] - 155s 2s/step - loss: 1.2307 - sparse_categorical_accuracy: 0.4842 - val_loss: 1.2054 - val_sparse_categorical_accuracy: 0.4447\n",
      "Epoch 5/25\n",
      "91/91 [==============================] - 153s 2s/step - loss: 1.1910 - sparse_categorical_accuracy: 0.5034 - val_loss: 1.3078 - val_sparse_categorical_accuracy: 0.4471\n",
      "Epoch 6/25\n",
      "91/91 [==============================] - 153s 2s/step - loss: 1.1637 - sparse_categorical_accuracy: 0.5182 - val_loss: 1.3117 - val_sparse_categorical_accuracy: 0.4135\n",
      "Epoch 7/25\n",
      "91/91 [==============================] - 153s 2s/step - loss: 1.1433 - sparse_categorical_accuracy: 0.5337 - val_loss: 1.2560 - val_sparse_categorical_accuracy: 0.4591\n",
      "Epoch 8/25\n",
      "91/91 [==============================] - 155s 2s/step - loss: 1.1297 - sparse_categorical_accuracy: 0.5309 - val_loss: 1.4422 - val_sparse_categorical_accuracy: 0.3678\n",
      "Epoch 9/25\n",
      "91/91 [==============================] - 153s 2s/step - loss: 1.0936 - sparse_categorical_accuracy: 0.5570 - val_loss: 1.2277 - val_sparse_categorical_accuracy: 0.4712\n",
      "Epoch 10/25\n",
      "91/91 [==============================] - 153s 2s/step - loss: 1.1160 - sparse_categorical_accuracy: 0.5398 - val_loss: 1.6371 - val_sparse_categorical_accuracy: 0.3221\n",
      "Epoch 11/25\n",
      "91/91 [==============================] - 153s 2s/step - loss: 1.1109 - sparse_categorical_accuracy: 0.5560 - val_loss: 1.2310 - val_sparse_categorical_accuracy: 0.4832\n",
      "Epoch 12/25\n",
      "91/91 [==============================] - 154s 2s/step - loss: 1.0519 - sparse_categorical_accuracy: 0.5697 - val_loss: 1.2584 - val_sparse_categorical_accuracy: 0.4495\n",
      "Epoch 13/25\n",
      "91/91 [==============================] - 153s 2s/step - loss: 1.0323 - sparse_categorical_accuracy: 0.5859 - val_loss: 1.2960 - val_sparse_categorical_accuracy: 0.4976\n",
      "Epoch 14/25\n",
      "91/91 [==============================] - 154s 2s/step - loss: 1.0218 - sparse_categorical_accuracy: 0.5958 - val_loss: 1.3077 - val_sparse_categorical_accuracy: 0.4904\n"
     ]
    }
   ],
   "source": [
    "history = model.fit(train_set, \n",
    "                    epochs=epochs, \n",
    "                    steps_per_epoch=int(steps_factor * max_train_instance / batch_size),\n",
    "                    validation_data=valid_set,\n",
    "                    validation_steps=int(max_valid_instance / batch_size),\n",
    "                    callbacks=callbacks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 212,
   "id": "3e870cac",
   "metadata": {},
   "outputs": [],
   "source": [
    "keras.backend.clear_session()\n",
    "np.random.seed(42)\n",
    "tf.random.set_seed(42)\n",
    "\n",
    "\n",
    "def cnn_bn_mp_r2_cnn_bn_r2_mp_de():\n",
    "    \"\"\"\n",
    "    name ecodes the model architecture\n",
    "    architecture:\n",
    "        first            CNN ---> BN ---> MP ---> repeated twice\n",
    "        second           CNN ---> CNN ---> BN ---> Mp repeared twice\n",
    "        dense            Dense ---> including dropout\n",
    "    \"\"\"\n",
    "\n",
    "    model = keras.models.Sequential()\n",
    "    model.add(keras.layers.Conv2D(64, kernel_size=5, \n",
    "                                  strides=1,  \n",
    "                                  use_bias=False, \n",
    "                                  input_shape=[dims[0], dims[0], channels], \n",
    "                                  ))\n",
    "    model.add(keras.layers.BatchNormalization())\n",
    "    model.add(keras.layers.Activation(\"relu\"))\n",
    "    model.add(keras.layers.MaxPool2D(pool_size=2, padding=\"same\"))\n",
    "    model.add(keras.layers.Conv2D(128, kernel_size=3, \n",
    "                                  strides=1,  \n",
    "                                  use_bias=False, \n",
    "                                  ))\n",
    "    model.add(keras.layers.BatchNormalization())\n",
    "    model.add(keras.layers.Activation(\"relu\"))\n",
    "    model.add(keras.layers.MaxPool2D(pool_size=2, padding=\"same\"))\n",
    "    model.add(keras.layers.Conv2D(256, kernel_size=3, \n",
    "                                  strides=1,  \n",
    "                                  use_bias=False, \n",
    "                                  ))\n",
    "    model.add(keras.layers.BatchNormalization())\n",
    "    model.add(keras.layers.Activation(\"relu\"))\n",
    "    model.add(keras.layers.Conv2D(256, kernel_size=3, \n",
    "                                  strides=1,  \n",
    "                                  use_bias=False, \n",
    "                                  ))\n",
    "    model.add(keras.layers.BatchNormalization())\n",
    "    model.add(keras.layers.Activation(\"relu\"))\n",
    "    model.add(keras.layers.MaxPool2D(pool_size=2, padding=\"same\"))\n",
    "    model.add(keras.layers.Conv2D(512, kernel_size=3, \n",
    "                                  strides=1,  \n",
    "                                  use_bias=False, \n",
    "                                  ))\n",
    "    model.add(keras.layers.BatchNormalization())\n",
    "    model.add(keras.layers.Activation(\"relu\"))\n",
    "    model.add(keras.layers.Conv2D(512, kernel_size=3, \n",
    "                                  strides=1,  \n",
    "                                  use_bias=False, \n",
    "                                  ))\n",
    "    model.add(keras.layers.BatchNormalization())\n",
    "    model.add(keras.layers.Activation(\"relu\"))\n",
    "    model.add(keras.layers.MaxPool2D(pool_size=2, padding=\"same\"))\n",
    "    model.add(keras.layers.Flatten())\n",
    "    model.add(keras.layers.Dropout(0.3))\n",
    "    model.add(keras.layers.Dense(64, activation=\"relu\"))\n",
    "    model.add(keras.layers.Dropout(0.3))\n",
    "    model.add(keras.layers.Dense(5, activation=\"softmax\"))\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 213,
   "id": "9465719b",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = cnn_bn_mp_r2_cnn_bn_r2_mp_de()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 214,
   "id": "01a9c8e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = keras.optimizers.Nadam()\n",
    "model.compile(loss=keras.losses.sparse_categorical_crossentropy, \n",
    "              optimizer=optimizer,\n",
    "              metrics=keras.metrics.sparse_categorical_accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 215,
   "id": "a83b79a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_target = os.path.join(BASE_DIR, \"models\", \"cnn_bn_mp_r2_cnn_bn_r2_mp_de.h5\")\n",
    "\n",
    "checkpoint_cb = keras.callbacks.ModelCheckpoint(model_target, save_best_only=True)\n",
    "early_stop_cb = keras.callbacks.EarlyStopping(patience=10)\n",
    "\n",
    "callbacks = [checkpoint_cb, early_stop_cb]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 216,
   "id": "b2591518",
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs = 25\n",
    "max_learning_rate = 0.001\n",
    "onecycle_cb = OneCycleSchedulerNadam(int(steps_factor * max_train_instance / batch_size) * epochs, max_lrate=max_learning_rate)\n",
    "\n",
    "callbacks.append(onecycle_cb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ba99ba5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/25\n",
      "91/91 [==============================] - 408s 4s/step - loss: 1.5246 - sparse_categorical_accuracy: 0.3668 - val_loss: 1.4680 - val_sparse_categorical_accuracy: 0.3317\n",
      "Epoch 2/25\n",
      "91/91 [==============================] - 407s 4s/step - loss: 1.3742 - sparse_categorical_accuracy: 0.3788 - val_loss: 1.3115 - val_sparse_categorical_accuracy: 0.4014\n",
      "Epoch 3/25\n",
      "91/91 [==============================] - 442s 5s/step - loss: 1.3344 - sparse_categorical_accuracy: 0.4193 - val_loss: 1.3736 - val_sparse_categorical_accuracy: 0.4279\n",
      "Epoch 4/25\n",
      "91/91 [==============================] - 447s 5s/step - loss: 1.3460 - sparse_categorical_accuracy: 0.4210 - val_loss: 1.3581 - val_sparse_categorical_accuracy: 0.4519\n",
      "Epoch 5/25\n",
      "91/91 [==============================] - 435s 5s/step - loss: 1.3063 - sparse_categorical_accuracy: 0.4299 - val_loss: 1.3861 - val_sparse_categorical_accuracy: 0.4688\n",
      "Epoch 6/25\n",
      "91/91 [==============================] - 448s 5s/step - loss: 1.3013 - sparse_categorical_accuracy: 0.4468 - val_loss: 1.3010 - val_sparse_categorical_accuracy: 0.5120\n",
      "Epoch 7/25\n",
      "91/91 [==============================] - 457s 5s/step - loss: 1.3155 - sparse_categorical_accuracy: 0.4457 - val_loss: 1.3258 - val_sparse_categorical_accuracy: 0.4712\n",
      "Epoch 8/25\n",
      "91/91 [==============================] - 447s 5s/step - loss: 1.3057 - sparse_categorical_accuracy: 0.4571 - val_loss: 1.4422 - val_sparse_categorical_accuracy: 0.3894\n",
      "Epoch 9/25\n",
      "91/91 [==============================] - 414s 5s/step - loss: 1.3865 - sparse_categorical_accuracy: 0.4688 - val_loss: 1.3060 - val_sparse_categorical_accuracy: 0.5024\n",
      "Epoch 10/25\n",
      "91/91 [==============================] - 413s 5s/step - loss: 1.3736 - sparse_categorical_accuracy: 0.4643 - val_loss: 1.2182 - val_sparse_categorical_accuracy: 0.5024\n",
      "Epoch 11/25\n",
      "91/91 [==============================] - 448s 5s/step - loss: 1.3221 - sparse_categorical_accuracy: 0.4595 - val_loss: 1.2675 - val_sparse_categorical_accuracy: 0.4327\n",
      "Epoch 12/25\n",
      "91/91 [==============================] - 428s 5s/step - loss: 1.2841 - sparse_categorical_accuracy: 0.4736 - val_loss: 1.1224 - val_sparse_categorical_accuracy: 0.5216\n",
      "Epoch 13/25\n",
      "91/91 [==============================] - 462s 5s/step - loss: 1.2342 - sparse_categorical_accuracy: 0.4900 - val_loss: 1.0734 - val_sparse_categorical_accuracy: 0.5481\n",
      "Epoch 14/25\n",
      "91/91 [==============================] - 478s 5s/step - loss: 1.1813 - sparse_categorical_accuracy: 0.5161 - val_loss: 1.0554 - val_sparse_categorical_accuracy: 0.5721\n",
      "Epoch 15/25\n",
      "91/91 [==============================] - 470s 5s/step - loss: 1.2027 - sparse_categorical_accuracy: 0.5096 - val_loss: 1.0998 - val_sparse_categorical_accuracy: 0.5697\n",
      "Epoch 16/25\n",
      "91/91 [==============================] - 408s 4s/step - loss: 1.1533 - sparse_categorical_accuracy: 0.5343 - val_loss: 1.0604 - val_sparse_categorical_accuracy: 0.5986\n",
      "Epoch 17/25\n",
      "91/91 [==============================] - 403s 4s/step - loss: 1.1372 - sparse_categorical_accuracy: 0.5158 - val_loss: 1.0928 - val_sparse_categorical_accuracy: 0.5745\n",
      "Epoch 18/25\n",
      "91/91 [==============================] - 400s 4s/step - loss: 1.1299 - sparse_categorical_accuracy: 0.5364 - val_loss: 0.9435 - val_sparse_categorical_accuracy: 0.6178\n",
      "Epoch 19/25\n",
      "52/91 [================>.............] - ETA: 2:32 - loss: 1.0947 - sparse_categorical_accuracy: 0.5595"
     ]
    }
   ],
   "source": [
    "history = model.fit(train_set, \n",
    "                    epochs=epochs, \n",
    "                    steps_per_epoch=int(steps_factor * max_train_instance / batch_size),\n",
    "                    validation_data=valid_set,\n",
    "                    validation_steps=int(max_valid_instance / batch_size),\n",
    "                    callbacks=callbacks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f800835",
   "metadata": {},
   "outputs": [],
   "source": [
    "def googlenet_mini():\n",
    "    \"\"\"\n",
    "    name ecodes the model architecture\n",
    "    architecture:\n",
    "        first            CNN ---> MP ---> LRN ---> Repeated twice\n",
    "        second           IL ---> IL ---> MP ---> IL ---> IL\n",
    "        third            GAP\n",
    "        fourth            Dense ---> including dropout\n",
    "    \"\"\"\n",
    "\n",
    "    model = keras.models.Sequential()\n",
    "    model.add(keras.layers.Conv2D(64, \n",
    "                                  kernel_size=5, \n",
    "                                  strides=2,  \n",
    "                                  use_bias=False,\n",
    "                                  padding=\"same\", \n",
    "                                  input_shape=[dims[0], dims[0], channels], \n",
    "                                  ))\n",
    "    model.add(keras.layers.Activation(\"relu\"))\n",
    "    model.add(keras.layers.MaxPool2D(pool_size=3, \n",
    "                                     strides=2,\n",
    "                                     padding=\"same\"))\n",
    "    \n",
    "    model.add(keras.layers.Lambda(lambda inputs: tf.nn.local_response_normalization(inputs)))\n",
    "    \n",
    "    model.add(keras.layers.Conv2D(64, \n",
    "                                  kernel_size=1, \n",
    "                                  strides=1,  \n",
    "                                  use_bias=False,\n",
    "                                  padding=\"same\", \n",
    "                                  ))\n",
    "    model.add(keras.layers.Activation(\"relu\"))\n",
    "    model.add(keras.layers.Conv2D(192, \n",
    "                                  kernel_size=3, \n",
    "                                  strides=1,  \n",
    "                                  use_bias=False,\n",
    "                                  padding=\"same\", \n",
    "                                  ))\n",
    "    model.add(keras.layers.Activation(\"relu\"))\n",
    "    \n",
    "    model.add(keras.layers.Lambda(lambda inputs: tf.nn.local_response_normalization(inputs)))\n",
    "\n",
    "    model.add(keras.layers.MaxPool2D(pool_size=3, \n",
    "                                     strides=2,\n",
    "                                     padding=\"same\"))\n",
    "\n",
    "\n",
    "    model.add(InceptionLayer([64, 128, 32, 32, 96, 16]))\n",
    "    model.add(InceptionLayer([128, 192, 96, 94, 128, 32]))\n",
    "    model.add(keras.layers.MaxPool2D(pool_size=3, \n",
    "                                     strides=2,\n",
    "                                     padding=\"same\"))\n",
    "    model.add(InceptionLayer([192, 208, 48, 94, 96, 16]))\n",
    "    model.add(InceptionLayer([160, 224, 64, 64, 112, 24]))\n",
    "    model.add(keras.layers.GlobalAveragePooling2D())\n",
    "    model.add(keras.layers.Dropout(0.1))\n",
    "    \n",
    "    model.add(keras.layers.Dense(5, activation=\"softmax\"))\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7897a0fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = googlenet_mini()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4e9257a",
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = keras.optimizers.Nadam()\n",
    "model.compile(loss=keras.losses.sparse_categorical_crossentropy, \n",
    "              optimizer=optimizer,\n",
    "              metrics=keras.metrics.sparse_categorical_accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "104e961a",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_target = os.path.join(BASE_DIR, \"models\", \"googlenet_mini.h5\")\n",
    "\n",
    "checkpoint_cb = keras.callbacks.ModelCheckpoint(model_target, save_best_only=True)\n",
    "early_stop_cb = keras.callbacks.EarlyStopping(patience=10)\n",
    "\n",
    "callbacks = [checkpoint_cb, early_stop_cb]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d68039b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs = 25\n",
    "max_learning_rate = 0.001\n",
    "onecycle_cb = OneCycleSchedulerNadam(int(steps_factor * max_train_instance / batch_size) * epochs, max_lrate=max_learning_rate)\n",
    "\n",
    "callbacks.append(onecycle_cb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c909eaf",
   "metadata": {},
   "outputs": [],
   "source": [
    "history = model.fit(train_set, \n",
    "                    epochs=epochs, \n",
    "                    steps_per_epoch=int(steps_factor * max_train_instance / batch_size),\n",
    "                    validation_data=valid_set,\n",
    "                    validation_steps=int(max_valid_instance / batch_size),\n",
    "                    callbacks=callbacks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d8b0e02",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20f3bb93",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4062ea7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fbf04799",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
